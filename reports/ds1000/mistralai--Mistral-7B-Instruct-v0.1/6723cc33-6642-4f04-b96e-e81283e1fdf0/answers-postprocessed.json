{
    "Pandas": [
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nList = np.random.permutation(len(df))\ndf = df.iloc[List]\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nList = np.random.permutation(len(df))\ndf = df.iloc[List]\nresult = df.groupby('Type')['Col1'].count().reset_index()\nresult = result.rename(columns={'Col1': 'Count'})\nresult = result.groupby('Type')['Count'].sum()\nprint(result)\n",
        "\nfor col in ['Qu1', 'Qu2', 'Qu3']:\n    df[col] = df[col].replace({'apple': 'other', 'egg': 'other'})\ndf['Qu1'] = df['Qu1'].str.replace('cheese', 'cheese')\ndf['Qu1'] = df['Qu1'].str.replace('banana', 'banana')\ndf['Qu1'] = df['Qu1'].str.replace('potato', 'potato')\ndf['Qu2'] = df['Qu2'].str.replace('banana', 'banana')\ndf['Qu2'] = df['Qu2'].str.replace('apple', 'banana')\ndf['Qu3'] = df['Qu3'].str.replace('apple', 'other')\ndf['Qu3'] = df['Qu3'].str.replace('egg', 'other')\n",
        "\nfor col in ['Qu1', 'Qu2', 'Qu3']:\n    df[col] = df[col].replace({'potato': 'other', 'banana': 'other', 'apple': 'other', 'egg': 'other'})\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\ndef f(df=example_df):\n    df['Qu1'] = df['Qu1'].str.replace('apple', 'other')\n    df['Qu1'] = df['Qu1'].str.replace('egg', 'other')\n    df['Qu2'] = df['Qu2'].str.replace('apple', 'other')\n    df['Qu3'] = df['Qu3'].str.replace('apple', 'other')\n    df['Qu3'] = df['Qu3'].str.replace('egg', 'other')\n    return df\n",
        "\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x in ['potato', 'banana', 'apple', 'egg'] and x.value_counts() >= 3 else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if x in ['potato', 'banana', 'apple', 'egg'] and x.value_counts() >= 2 else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if x in ['potato', 'banana', 'apple', 'egg'] and x.value_counts() >= 2 else x)\n",
        "\ndf['Qu1'] = df['Qu1'].str.replace('apple', 'other')\ndf['Qu1'] = df['Qu1'].str.replace('egg', 'other')\ndf['Qu1'] = df['Qu1'].str.replace('potato', 'other')\ndf['Qu1'] = df['Qu1'].str.replace('banana', 'other')\ndf['Qu1'] = df['Qu1'].str.replace('cheese', 'cheese')\ndf['Qu1'] = df['Qu1'].str.replace('other', 'cheese')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n# Keep first occurence of \"url\" field, BUT  keep duplicates if the field \"keep_if_dup\" is YES\nresult = df.drop_duplicates(subset='url', keep=lambda x: x['keep_if_dup'] == 'Yes')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'drop_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n# Keep first occurence of \"url\" field, BUT keep duplicates if the field \"drop_if_dup\" is No.\nresult = df.drop_duplicates(subset=['url', 'drop_if_dup'], keep=['first', 'drop_if_dup'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n# Keep last occurence of \"url\" field, BUT keep duplicates if the field \"keep_if_dup\" is YES\nresult = df.groupby('url')['keep_if_dup'].last().reset_index()\n",
        "\nresult = {}\nfor name, values in df.groupby('name'):\n    nested_dict = {}\n    for value in values:\n        nested_dict[value['v1']] = {}\n        for key, value in value.groupby('v2'):\n            nested_dict[value['v1']][key] = value['v3']\n    result[name] = nested_dict\n",
        "\ndf['datetime'] = df['datetime'].astimezone(pd.Timestamp('2022-01-01 00:00:00'))\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\nexample_df['datetime'] = pd.to_datetime(example_df['datetime'])\ndef f(df=example_df):\n    df['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    return df\n",
        "\ndf['datetime'] = df['datetime'].dt.strftime('%d-%m-%Y %H:%M:%S')\n",
        "\ndf['datetime'] = df['datetime'].astimezone(pd.Timestamp('2022-01-01 00:00:00'))\n",
        "\n# Extract the key value pairs from the message column\nmessage_list = df['message'].apply(lambda x: x.split(',')).tolist()\n# Create a dictionary to store the key value pairs\nkey_value_dict = {}\n# Loop through the message list and extract the key value pairs\nfor message in message_list:\n    key_value_dict.update({k: v.strip() for k, v in message.split(': ')})\n# Convert the dictionary to a dataframe\nresult = pd.DataFrame(key_value_dict.items())\n# Rename the columns\nresult.columns = ['name', 'status', 'number', 'job', 'money', 'wife', 'group', 'kids']\n# Merge the result dataframe with the original dataframe\nresult = pd.merge(result, df, on=['name', 'status', 'number'])\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10\n",
        "\ndf.loc[(df['product'] >= products[0][0] & df['product'] <= products[0][1]) | (df['product'] >= products[1][0] & df['product'] <= products[1][1]), 'score'] = df.loc[(df['product'] >= products[0][0] & df['product'] <= products[0][1]) | (df['product'] >= products[1][0] & df['product'] <= products[1][1]), 'score'] * 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'].apply(lambda x: (x - df.loc[df['product'].isin(products), 'score'].min()) / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min()))\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 0, 1],\n                   'C': [0, 0, 1, 0, 0, 0],\n                   'D': [0, 0, 0, 1, 0, 0]})\n# [Missing Code]\ndf['category'] = df[['A', 'B', 'C', 'D']].apply(lambda x: x.apply(lambda y: 'A' if y == 1 else '0'))\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': [0, 1, 1, 1, 0, 1],\n                   'B': [1, 0, 1, 1, 1, 0],\n                   'C': [1, 1, 0, 1, 1, 1],\n                   'D': [1, 1, 1, 0, 1, 1]})\n# [Missing Code]\ndf['category'] = df.apply(lambda row: 'A' if row['A'] == 0 else 'B' if row['B'] == 0 else 'C' if row['C'] == 0 else 'D', axis=1)\n",
        "\ndf['category'] = df[['A', 'B', 'C', 'D']].apply(lambda x: list(x[x == 1].values), axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.to_period(\"M\")\ndf['Date'] = df['Date'].dt.strftime(\"%B-%Y\")\n",
        "\ndf['Date'] = df['Date'].dt.to_period(\"M\")\ndf['Date'] = df['Date'].dt.strftime(\"%d-%b-%Y\")\n",
        "\nresult = df[(df['Date'] >= List[0]) & (df['Date'] <= List[1])]\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date'] = result['Date'].str.replace(' ', '-')\nresult['Date'] = result['Date'].str.replace('-', ' ')\nresult['Date']",
        "\ndf = df.shift(1, axis=0)\ndf = df.iloc[0:4, :]\ndf = df.iloc[4:, :]\ndf = df.iloc[0:4, :]\ndf = df.iloc[4:, :]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n# [Missing Code]\ndf = df.iloc[:-1].append(df.iloc[-1].values.reshape(1, -1), axis=0)\ndf = df.iloc[::-1]\n",
        "\ndf.iloc[0, 0] = df.iloc[1, 0]\ndf.iloc[0, 1] = df.iloc[-1, 1]\ndf.iloc[-1, 0] = df.iloc[0, 0]\ndf.iloc[-1, 1] = df.iloc[0, 1]\nresult = df\n",
        "\n# Shift the first row of the first column down 1 row\ndf.iloc[0, 0] = df.iloc[1, 0]\n# Shift the last row of the first column up 1 row\ndf.iloc[-1, 0] = df.iloc[0, 0]\n# Rearrange the first and second columns\ndf = df[['#2', '#1']]\n# Compute the R^2 values of the first and second columns\nr2_values = []\nfor i in range(len(df)):\n    X = df.iloc[i, 0].values.reshape(-1, 1)\n    y = df.iloc[i, 1].values.reshape(-1, 1)\n    r2_values.append(r2)\n# Find the index of the minimum R^2 value\nmin_r2_index = np.argmin(r2_values)\n# Rearrange the first and second columns based on the minimum R^2 value\ndf = df[['#2', '#1']]\ndf = df[['#2', '#1']].iloc[min_r2_index, :]\n# Output the resulting DataFrame\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n# Rename all columns by adding \"X\" at the end\ndf.columns = df.columns + \"X\"\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n# Rename all columns by adding \"X\" to the beginning\ndf = df.rename(columns={'HeaderA': 'XHeaderA', 'HeaderB': 'XHeaderB', 'HeaderC': 'XHeaderC'})\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457],\n     \"HeaderX\": [345]})\n# Rename all columns by adding \"X\" to the end of the column name\ndf.columns = df.columns + \"X\"\n# Concatenate multiple dataframes and add a suffix to the column names\nresult = pd.concat([df, pd.DataFrame(columns=df.columns + \"_1\"), pd.DataFrame(columns=df.columns + \"_2\")], axis=1)\nresult.columns = result.columns[:-3] + \"_\" + result.columns[-3:]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\", \"val3\": \"mean\"})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", \"val3\": \"sum\"})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val42':[1,1,4,5,1] })\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val42\": \"sum\", \"val*\": \"mean\"})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.loc[row_list, column_list].mean(axis=0)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.loc[row_list, column_list].sum(axis=0)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.loc[row_list, column_list].sum(axis=0).nlargest(len(column_list))\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nresult = df.value_counts().reset_index()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'null'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nresult = df.isnull().sum()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nresult = df.value_counts().reset_index()\nresult.columns = ['value', 'count']\nresult.groupby('columns')['count'].sum().reset_index()\nprint(result)\n",
        "\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)",
        "\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\ndf = df.iloc[:-1].append(df.iloc[-1], ignore_index=True)\ndf = df.reset_index(drop=True)",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\nresult = df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[3,1,2],[1,2,np.nan],[2,np.nan,np.nan]],columns=['0','1','2'])\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n# Create a new column with the sum of the rows smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x >= thresh else df.loc[df['value'] < thresh].sum())\n# Rename the column with the sum of the rows smaller than the threshold\ndf.columns = ['value', 'lab']\n# Drop the rows with the sum of the rows smaller than the threshold\ndf = df.loc[df['value'] >= thresh]\n# Print the result\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n# [Missing Code]\n# Create a new column 'value' with the average of the rows whose value is greater than the threshold\ndf['value'] = df.loc[df['value'] > thresh].mean()\n",
        "\nresult = df.loc[df['value'] < section_left | df['value'] > section_right].mean()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nresult = df.apply(lambda x: pd.Series(1/x))\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nresult = df.apply(lambda x: pd.Series(np.exp(x), index=x.index, name=f\"exp_{x.name}\"))\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [4, 5, 6]})\nresult = df.apply(lambda x: pd.Series(1/x.values, index=x.index))\nprint(result)\n",
        "\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"sigmoid_A\": [1/(1+e^(-1)), 1/(1+e^(-2)), 1/(1+e^(-3))], \"sigmoid_B\": [1/(1+e^(-4)), 1/(1+e^(-5)), 1/(1+e^(-6))]})\n# [Missing Code]\n",
        "\ndf.loc[df.idxmin(axis=1).idxmax(axis=1)]\n",
        "\ndf.loc[df.idxmin(axis=1).idxmax(axis=0)]\n",
        "\n# Find the minimum and maximum date within the date column\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n# Expand the date column to have all the dates there while simultaneously filling in 0 for the val column\ndf = pd.concat([df, pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D'), 'user': df['user'], 'val': 0})], ignore_index=True)\n",
        "\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = df['dt'].unique()\ndf = df.set_index('dt')\ndf['user'] = df['user'].repeat(len(df['dt']))\ndf['val'] = df['val'].repeat(len(df['dt']))\ndf = df.reset_index()\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n# Find the minimum and maximum date within the date column\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n# Expand the date column to have all the dates there while simultaneously filling in 233 for the val column\ndf = pd.concat([df, pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D'), 'user': df['user'], 'val': 233})], ignore_index=True)\nprint(df)\n",
        "\n# Find the minimum and maximum dates within the date column\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n# Expand the date column to have all the dates there\ndf['dt'] = pd.date_range(min_dt, max_dt, freq='D')\n# Fill in the maximum val of the user for the val column\ndf['val'] = df.groupby('user')['val'].max()\n",
        "\n# Find the minimum and maximum date within the date column\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n# Expand the date column to have all the dates there\ndf['dt'] = pd.date_range(min_dt, max_dt, freq='D')\n# Fill in the maximum val of the user for the val column\ndf['val'] = df.groupby('user')['val'].max()\n# Convert df to the desired format\ndf['dt'] = df['dt'].dt.strftime('%d-%b-%Y')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n# Replace each name with a unique ID\ndf['id'] = df['name'].apply(lambda x: x.split()[0])\ndf['name'] = df['name'].apply(lambda x: x.split()[1:])\n",
        "\ndf['a'] = df['name'].apply(lambda x: x if x != 'Aaron' else x + 1)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\ndef f(df=example_df):\n    df['id'] = df['name'].apply(lambda x: x.split()[0])\n    return df\n",
        "\ndf['ID'] = df['name'] + df['a']\ndf['name'] = df['ID']\ndf = df.drop(['a', 'name'], axis=1)\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n# Pivot the table\nresult = pd.pivot_table(df, values='02/12/15', index='user', columns='someBool', aggfunc=lambda x: x.sum())\n# Rename columns\nresult.columns = ['value', 'others']\n# Print the result\nprint(result)\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\ndf_subset = df[(df['c'] > 0.5) & (df['c'] < 0.5)]\nsubset_columns = columns\nsubset_columns = subset_columns[subset_columns.isin(df_subset.columns)]\nsubset_df = df_subset[subset_columns]\nsubset_df = subset_df.values\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['a','b','e']\nresult = df[df.c > 0.45][columns]\nprint(result)\n",
        "\nimport pandas as pd\ndef f(df, columns=['b', 'e']):\n    # Select rows where column 'c' is greater than 0.5\n    df_subset = df[df.c > 0.5]\n    \n    # Select columns 'b' and 'e' from the subset\n    df_subset = df_subset[columns]\n    \n    # Convert the subset to a numpy array\n    result = df_subset.values\n    \n    return result\n",
        "\n    result = df[df.c > 0.5][columns]\n    result['sum'] = result['b'] + result['e']\n    ",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    return df[df.c > 0.5][locs]\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.date\ndf['date_diff'] = df['date'].diff(periods=pd.DateOffset(days=X))\ndf['date_diff'].fillna(0, inplace=True)\ndf = df[(df['date_diff'] > 0) & (df['date_diff'].dt.days < X)]\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.date\ndf['date_diff'] = df['date'].diff(periods=pd.DateOffset(weeks=X))\ndf['date_diff'].fillna(0, inplace=True)\ndf = df[(df['date_diff'] > 0) & (df['date_diff'].dt.days < 7)]\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'])",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = df.groupby(range(0, len(df), 3))['col1'].mean()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1]})\nresult = df.groupby(range(0, len(df), 3))['col1'].apply(lambda x: x.mean())\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1, 4]})\nresult = df.groupby(df.index // 4).agg({'col1': 'count'})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = df.groupby(range(len(df)-2, -1, -3))['col1'].mean()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n# Begin of Missing Code\nresult = df.groupby(range(0, len(df), 3))['col1'].sum().reset_index()\nresult = result.groupby(range(0, len(result), 2))['col1'].mean().reset_index()\n# End of Missing Code\nprint(result)\n",
        "\nresult = df.groupby(range(0, len(df), 3))[['col1']].agg({'sum': 'sum', 'mean': 'mean'}).reset_index()\n",
        "\ndf.fillna(method='ffill', inplace=True)\n",
        "\nimport pandas as pd\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\ndf.fillna(method='ffill', inplace=True)\n",
        "\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].apply(lambda x: x.max() if x != 0 else x)\n",
        "\ndf['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\ndf['time'] = df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\ndf['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n",
        "\ndf['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\ndf['time'] = df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\ndf['time_day']= df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\ndef f(df=example_df):\n    df['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\n    df [ 'time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\n    df['time_day']= df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n    return df\n",
        "\ndf['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\ndf['time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\ndf['time_day']= df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day']*=df['number']\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\ncolumns_check_list = ['A','B','C','D','E','F']\nprint(result)\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\ncolumns_check_list = ['A','B','C','D','E','F']\nresult = np.where([df[column] == df[column] for column in columns_check_list])\nprint(result)\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\nimport pandas as pd\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                 names=('name', 'datetime'))\ndf = pd.DataFrame({'fee': [100, 90, 80], 'credits':[7, 8, 9]}, index=index)\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndef f(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index('date')\n    return df.values\n",
        "\nimport pandas as pd\ndef f(df):\n    df = df.set_index('date', inplace=True)\n    df = df.sort_index(ascending=False, inplace=True)\n    df = df.reset_index(drop=True, inplace=True)\n    df = df.rename(columns={'x': 'id', 'y': 'date'}, inplace=True)\n    return df\n",
        "\ndf = pd.melt(df,id_vars='Country',value_name='Var1', var_name='year')\ndf = pd.melt(df,id_vars='Country',value_name='Var2', var_name='year')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n                   'Variable': ['var1', 'var2', 'var1', 'var2'],\n                   '2000': [12, 1, 20, 0],\n                   '2001': [15, 3, 23, 1],\n                   '2002': [18, 2, 25, 2],\n                   '2003': [17, 5, 29, 2],\n                   '2004': [23, 7, 31, 3],\n                   '2005': [29, 5, 32, 3]})\ndf = pd.melt(df, id_vars=['Country', 'Variable'], value_name='Var1', var_name='year')\ndf = df.sort_values(by=['year', 'Country', 'Variable'], ascending=False)\ndf = df.rename(columns={'Var1': 'var1', 'Var2': 'var2'})\nresult = df\nprint(result)\n",
        "result = df[(df['Value_A'] < 1) & (df['Value_B'] < 1) & (df['Value_C'] < 1) & ... & (df['Value_N'] < 1)]\n",
        "result = df[(df['Value_A'].abs() > 1) | (df['Value_B'].abs() > 1) | (df['Value_C'].abs() > 1) | ... | (df['Value_N'].abs() > 1)]\n",
        "\nresult = df[(df['Value_B'].abs() > 1) | (df['Value_C'].abs() > 1) | (df['Value_D'].abs() > 1) | ... | (df['Value_N'].abs() > 1)]\nresult = result.drop(['Value_B', 'Value_C', 'Value_D', ..., 'Value_N'], axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\ndf['A'] = df['A'].str.replace('&AMP;', '&')\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': ['Good &LT bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &LT; bad'] * 5})\ndf['A'] = df['A'].str.replace('&LT;', '<')\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\ndef f(df=example_df):\n    df['A'] = df['A'].str.replace('&AMP;', '&')\n    return df\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;','&''<''>')\ndf['A'] = df['A'].str.replace('&LT;','<')\ndf['A'] = df['A'].str.replace('&GT;','>')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': ['1 &AMP; 1', 'BB', 'CC', 'DD', '1 &AMP; 0'], 'B': range(5), 'C': ['0 &AMP; 0'] * 5})\ndf['A'] = df['A'].str.replace('&AMP;', '&')\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n# Split the name column into first_name and last_name IF there is one space in the name. Otherwise shove the full name into first_name\ndf['first_name'] = df['name'].apply(lambda x: validate_single_space_name(x) if ' ' in x else x)\ndf['last_name'] = df['name'].apply(lambda x: validate_single_space_name(x) if ' ' in x else None)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n# Split the name column into 1_name and 2_name IF there is one space in the name. Otherwise shove the full name into 1_name\ndf['1_name'] = df['name'].apply(lambda x: validate_single_space_name(x) if ' ' in x else x)\ndf['2_name'] = df['name'].apply(lambda x: x.split()[1:] if ' ' in x else None)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\nresult = df['name'].apply(validate_single_space_name)\nresult = result.str.split(' ', expand=True)\nresult = result.rename(columns={'first_name': 'first name', 'last_name': 'last_name', 'middle_name': 'middle_name'})\nprint(result)\n",
        "\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\nresult = pd.merge(df1, df2, on='Timestamp')\nprint(result)\n",
        "\ndf1 = pd.merge(df1, df2, on='Timestamp')\n",
        "\ndf['state'] = df[['col1', 'col2', 'col3']].apply(lambda x: x.max() if x['col2'] <= x['col3'] <= 50 else x['col1'], axis=1)\n",
        "\ndf['state'] = df['col1'] + df['col2'] + df['col3']\ndf.loc[(df['col2'] > 50) & (df['col3'] > 50), 'state'] = df['col1']\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n# Iterate over each row and check if Field1 is integer\nfor index, row in df.iterrows():\n    if not row[\"Field1\"].is_integer():\n        result.append(row[\"Field1\"])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n# Iterate over each row and check if Field1 is integer\nresult = []\nfor index, row in df.iterrows():\n    if row[\"Field1\"].is_integer():\n        result.append(row[\"Field1\"])\nprint(result)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    result = []\n    for index, row in df.iterrows():\n        if not row[\"Field1\"].isnumeric():\n            result.append(row[\"Field1\"])\n    return result\n",
        "\nresult = df.groupby('cat')['val1', 'val2', 'val3', 'val4'].sum().div(df.groupby('cat').sum().sum(), axis=1).reset_index()\nresult.columns = ['cat', 'val1', 'val2', 'val3', 'val4']\n",
        "\nresult = df.groupby('cat')['val1', 'val2', 'val3', 'val4'].sum().div(df.groupby('cat')['val1', 'val2', 'val3', 'val4'].sum().sum(), axis=1).reset_index()\nresult.columns = ['cat', 'val1', 'val2', 'val3', 'val4']\n",
        "\ndf.loc[test]\n",
        "\ndf.loc[test]\n",
        "\ndf.drop(test, axis=0, inplace=True)\n",
        "\nimport pandas as pd\ndef f(df, test):\n    result = df.loc[test]\n    return result\n",
        "\nimport pandas as pd\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# Calculate pairwise distances between cars\ndistances = df.pivot_table(index='time', columns='car', values='euclidean_distance')\n# Get nearest neighbour for each car\nnearest_neighbours = distances.groupby('car').apply(lambda x: x.idxmin())\n# Calculate average distance for each time point\nresult = df.groupby('time')['euclidean_distance'].mean()\nresult = result.reset_index()\nresult = result.rename(columns={'euclidean_distance': 'average_distance'})\nresult = result.join(nearest_neighbours)\nprint(result)\n",
        "\ndf['farmost_neighbour'] = df.groupby('car')['x'].apply(lambda x: x.idxmax())\ndf['euclidean_distance'] = df.apply(lambda x: ((x['x'] - x['farmost_neighbour']['x'])**2 + (x['y'] - x['farmost_neighbour']['y'])**2)**0.5, axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \",\".join(cols), axis=1)\nresult = df\nprint(result)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\ncols = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']]\ndf['keywords_all'] = cols.apply(lambda x: '-'.join(x), axis=1)\n",
        "\ncols = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']]\ndf['keywords_all'] = cols.apply(lambda x: '-'.join(x), axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n# Set random_state to 0\ndf.set_random_state(0)\n# Select 20% of rows randomly\nresult = df.sample(n=int(len(df)*0.2), random_state=0)\n# Change the value of the Quantity column of these rows to zero\nresult['Quantity'] = 0\n# Keep the indexes of the altered rows\nresult.index = df.index\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n# Set random_state to 0\ndf.set_random_state(0)\n# Select 20% of rows randomly\nresult = df.sample(n=int(len(df)*0.2), random_state=0)\n# Replace ProductId with 0 in the selected rows\nresult['ProductId'] = result['ProductId'].replace([4, 7], 0)\n# Keep the indexes of the altered rows\nresult.index = result.index[:len(result)]\nprint(result)\n",
        "\nresult = df.sample(frac=0.2, random_state=0)\nresult.loc[result['UserId'] == 1, 'Quantity'] = 0\nresult.loc[result['UserId'] == 2, 'Quantity'] = 0\nresult.loc[result['UserId'] == 3, 'Quantity'] = 0\n",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\n# Begin of Missing Code\n# Create a new column 'index_original' with the index of the first duplicate (the one kept)\ndf['index_original'] = df.groupby(['col1','col2']).idxmin()\n# End of Missing Code\nprint(df)\n",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\n# Begin of Missing Code\n# Create a new column 'index_original' with the index of the last duplicate\ndf['index_original'] = df.groupby(['col1','col2']).transform('last').reset_index(name='index_original')\n# End of Missing Code\nprint(df)\n",
        "\nimport pandas as pd\ndef f(df=example_df):\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = df.index[duplicate_bool == True]\n    return duplicate\n",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\n# [Missing Code]\n# Create a new column 'index_original' with the index of the first duplicate (the one kept)\ndf['index_original'] = df.groupby(['col1','col2','3col']).transform('first').reset_index(name='index_original')\n",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\n# [Missing Code]\nresult = df.duplicated(subset=['col1','col2'], keep='last')\nresult['index_original'] = result.index\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nresult = result[result['count'] == result['count'].max()]\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].min().reset_index()\nresult = result.merge(df, on=['Sp','Mt'])\n",
        "\nresult = df.groupby(['Sp','Value'])['count'].max().reset_index()\nresult = result[result['count'] == result['count'].max()]\n",
        "\nimport pandas as pd\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\nresult = df.query(\"Category==\" + filter_list)\nprint(result)\n",
        "\nimport pandas as pd\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\nresult = df.query(\"Category!=filter_list\")\nprint(result)\n",
        "\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\ndf.columns = df.columns.map(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf.columns = df.columns.apply(lambda x: list(x))\ndf",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n# Create a list of tuples where the first element is the first column level,\n# the second is the second column level, and the third element is the third column level\nvalue_vars = [(df.columns[0], df.columns[1], df.columns[2])]\n# Use pd.melt to melt the data frame\nresult = pd.melt(df, id_vars=df.columns, value_vars=value_vars)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\nprint(df)\nresult = df\n",
        "\nresult = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\ndf['cummax'] = df.groupby('id')['val'].cummax()\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].apply(lambda x: x if x >= 0 else 0)\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\nresult = df.groupby('l')['v'].sum().fillna(np.nan)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\nresult = df.groupby('r')['v'].sum().fillna(np.nan)\nprint(result)\n",
        "\n# df.groupby('l')['v'].sum()['right']\n",
        "\ndef get_relationship(df):\n    result = []\n    for i in range(df.shape[1]):\n        for j in range(i+1, df.shape[1]):\n            if df.iloc[:,i].unique().size == 1 and df.iloc[:,j].unique().size == 1:\n                result.append(f'Column{i} Column{j} one-to-one')\n            elif df.iloc[:,i].unique().size == 1:\n                result.append(f'Column{i} Column{j} one-to-many')\n            elif df.iloc[:,j].unique().size == 1:\n                result.append(f'Column{i} Column{j} one-to-many')\n            else:\n                result.append(f'Column{i} Column{j} many-to-many')\n    return result\n",
        "\ndef get_relationship(df):\n    relationships = []\n    for i in range(df.shape[1]):\n        for j in range(i+1, df.shape[1]):\n            if df.iloc[:,i].unique().size == 1 and df.iloc[:,j].unique().size == 1:\n                relationships.append(f'Column{i} Column{j} one-2-one')\n            elif df.iloc[:,i].unique().size == 1:\n                relationships.append(f'Column{i} Column{j} one-2-many')\n            elif df.iloc[:,j].unique().size == 1:\n                relationships.append(f'Column{i} Column{j} many-2-one')\n            else:\n                relationships.append(f'Column{i} Column{j} many-2-many')\n    return relationships\nresult = get_relationship(df)\n",
        "\ndef get_relationship(df):\n    result = df.apply(lambda x: pd.Series(x.apply(lambda y: (y.value_counts().index, y.value_counts().index))), axis=1)\n    result.columns = ['Column' + str(i) for i in range(1, len(df.columns) + 1)]\n    result.index = result.index.astype(str)\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace('Column', '')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')\n    result.columns = result.columns.str.replace('-', '_')\n    result.columns = result.columns.str.replace('_', ' ')\n    result.columns = result.columns.str.replace(' ', '-')",
        "\ndef get_relationship(df):\n    result = df.copy()\n    result['Column1'] = result['Column1'].apply(lambda x: 'one-2-many' if len(set(result[result['Column1'] == x]['Column2'])) > 1 else 'one-2-one')\n    result['Column2'] = result['Column2'].apply(lambda x: 'many-2-one' if len(set(result[result['Column2'] == x]['Column1'])) > 1 else 'many-2-many')\n    result['Column3'] = result['Column3'].apply(lambda x: 'many-2-one' if len(set(result[result['Column3'] == x]['Column2'])) > 1 else 'many-2-many')\n    result['Column4'] = result['Column4'].apply(lambda x: 'one-2-one' if len(set(result[result['Column4'] == x]['Column3'])) > 1 else 'one-2-many')\n    result['Column5'] = result['Column5'].apply(lambda x: 'many-2-many' if len(set(result[result['Column5'] == x]['Column3'])) > 1 else 'many-2-many')\n    return result\n",
        "\n# Get the index of unique values, based on firstname, lastname, email\n# Convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n",
        "\ndf = pd.DataFrame(s)\ndf.astype(str).str.replace(',','')\n",
        "\ndf.groupby(['Survived', (df['SibSp'] > 0) | (df['Parch'] > 0)])['Survived'].mean()\n",
        "\ndf.groupby(['Survived', 'Parch'])['SibSp'].mean()\n",
        "\ndf.groupby(['SibSp', 'Parch'])['Survived'].mean()\n",
        "\nresult = df.groupby('cokey').sort('A')\n",
        "\nresult = df.groupby('cokey').sort('A')\n",
        "\nimport pandas as pd\nimport numpy as np\nl = [('A', 'a'),  ('A', 'b'), ('B','a'),  ('B','b')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 4), columns=l)\n# Create a MultiIndex from the tuple column headers\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'A', 'B'])\n# Pivot the DataFrame to the desired format\nresult = df.pivot_table(index='index', columns='Caps', values='Value', aggfunc=np.sum)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nl = [('A', '1', 'a'),  ('A', '1', 'b'), ('A', '2', 'a'), ('A', '2', 'b'), ('B', '1','a'),  ('B', '1','b')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 6), columns=l)\n# Create a MultiIndex from the tuple column headers\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'A', 'B'])\n# Pivot the DataFrame to the desired format\nresult = df.pivot_table(index='Caps', columns='A', values='B')\n# Rename the columns to the desired names\nresult.columns = ['Middle', 'Lower']\nresult.columns.name = 'index'\n# Print the resulting DataFrame\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nl = [('A', 'a', '1'), ('A', 'b', '2'), ('B','a', '1'), ('A', 'b', '1'),  ('B','b', '1'),  ('A', 'a', '2')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 6), columns=l)\ndf.columns = [('Caps', 'A', 'B')]\ndf.columns.name = 'col_level_0'\ndf.columns.levels[0] = ['Middle', 'Lower']\ndf.columns.levels[1] = ['a', 'b']\ndf.columns.name = 'col_level_1'\ndf.columns.levels[1] = ['a', 'b']\ndf.columns.name = 'col_level_2'\nresult = df\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nnp.random.seed(123)\nbirds = np.random.choice(['African Swallow', 'Dead Parrot', 'Exploding Penguin'], size=int(5e4))\nsomeTuple = np.unique(birds, return_counts=True)\nresult = pd.DataFrame(someTuple, columns=['birdType', 'birdCount'])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\nresult = pd.Series(data.groupby('a').b.apply(lambda x: np.std(np.mean(x))))\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\nresult = pd.Series(data.groupby('b').a.apply(lambda x: np.std(np.mean(x))))\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n# Softmax normalization\ndf['softmax'] = df['b'] / df['b'].sum(axis=1, keepdims=True)\n# Min-max normalization\ndf['min-max'] = (df['b'] - df['b'].min(axis=1, keepdims=True)) / (df['b'].max(axis=1, keepdims=True) - df['b'].min(axis=1, keepdims=True))\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,1,0,1],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n# Remove rows and columns with only zeros\nresult = df.dropna()\n# Pivot the dataframe to get the desired output\nresult = result.pivot_table(index='A', columns='B', values='D')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[-1,-1,0,2],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n# Remove rows and columns with sum of 0\nresult = df.dropna(subset=['A','B','C','D'])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n# Remove rows and columns with max value of 2\nresult = df.dropna(subset=['A','B','C','D'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\nresult = df.where(df == df.max(), 0)\nprint(result)\n",
        "\nimport pandas as pd\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n          index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\nresult = s.sort_values(ascending=True, ignore_index=True)\nprint(result)\n",
        "\nimport pandas as pd\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n              index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\n# Sort the Series by value and index in ascending order\ns = s.sort_values()\n# Convert the Series to a DataFrame\nresult = s.to_frame()\n# Print the DataFrame\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n# Filter the dataframe to select only records where A value are integer or numeric\nresult = df[(df['A'].astype(int) != df['A']).all(axis=1)]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n# Filter the dataframe to select only the records where A value are string\nresult = df[df['A'].str.isalpha()]\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nresult = result[result['count'] == result['count'].max()]\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nresult = result[result['count'] == result['count'].max()]\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].min().reset_index()\nresult = result.merge(df, on=['Sp','Mt'])\n",
        "\nresult = df.groupby(['Sp','Value'])['count'].max().reset_index()\nresult = result[result['count'] == result['count'].max()]\n",
        "\ndf['Date'] = df['Date'].fillna(df['Member'])\ndf['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x, format='%m/%d/%Y'))\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
        "\ndf['Date'] = df['Date'].fillna(df['Member'])\ndf['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x) if x in dict else pd.Timestamp('17/8/1926'))\n",
        "\nimport pandas as pd\nexample_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\nexample_df = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\ndef f(dict=example_dict, df=example_df):\n    df['Date'] = df['Date'].fillna(df['Member'])\n    df['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x) if isinstance(x, str) else x)\n    df['Date'] = df['Date'].apply(lambda x: x.strftime('%Y/%m/%d'))\n    df['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x))\n    return df\n",
        "\ndf['Date'] = df['Date'].fillna(df['Member'])\ndf['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x, format='%d/%m/%Y'))\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
        "\ndf['Count_m'] = df.groupby(df['Date'].dt.month.rename('month')).size()\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year')).size()\n",
        "\ndf['Count_m'] = df.groupby(df['Date'].dt.month.rename('month')).size()\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year')).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.dt.month.map(df.groupby('Date').size())\ndf['Count_y'] = df.Date.dt.year.map(df.groupby('Date').size())\ndf['Count_w'] = df.Date.dt.weekday.map(df.groupby('Date').size())\ndf['Count_Val'] = df.groupby('Date').Val.size()\nresult = df\nprint(result)\n",
        "\nresult1 = df.groupby('Date')['B'].sum().reset_index()\nresult1['B'] = result1['B'].fillna(0)\nresult1 = result1.groupby('Date')['B'].sum().reset_index()\nresult2 = df.groupby('Date')['C'].sum().reset_index()\nresult2['C'] = result2['C'].fillna(0)\nresult2 = result2.groupby('Date')['C'].sum().reset_index()\n",
        "\nresult1 = df.groupby('Date')['B'].apply(lambda x: x[x % 2 == 0].sum())\nresult2 = df.groupby('Date')['C'].apply(lambda x: x[x % 2 == 1].sum())\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\n# [Missing Code]\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=['np.sum', 'np.mean'])\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1)\ndf = pd.DataFrame({\n'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n'B' : ['A', 'B', 'C'] * 4,\n'D' : np.random.randn(12),\n'E' : np.random.randn(12)\n})\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.max, np.min])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\nresult = df.apply(lambda x: pd.DataFrame(x['var2'].str.split(',').tolist(), columns=['var2']), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\nresult = df.apply(lambda x: pd.DataFrame(x['var2'].str.split(',').tolist(), columns=['var2']), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[\"A\", \"Z-Y\"], [\"B\", \"X\"], [\"C\", \"W-U-V\"]], index=[1,2,3], columns=['var1', 'var2'])\nresult = df.apply(lambda x: pd.DataFrame(x['var2'].str.split('-', expand=True), columns=['var2']), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n# [Missing Code]\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df.apply(count_special_char, axis = 0)\n",
        "\ndf[\"new\"] = df.apply(lambda x: sum(1 for c in x[\"str\"] if c.isalpha()), axis=0)\n",
        "\ndf['fips'] = df['row'].str[:-1]\ndf['row'] = df['row'].str[-1:]\n",
        "\ndf['fips'] = df['row'].str[:-1]\ndf['row'] = df['row'].str[-1:]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALAB AMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n# Split the row column into three columns\ndf = df.row.str.split(',').explode().reset_index(name='row')\ndf = df.rename(columns={'row': 'fips', 'row': 'medi', 'row': 'row'})\nresult = df\n",
        "\nresult = df.groupby('Name')['2001:2006'].apply(lambda x: x.fillna(0).mean())\n",
        "\nresult = df.groupby('Name')['2001:2006'].apply(lambda x: x.fillna(0).mean())\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\ndef f(df=example_df):\n    df['cumulative_avg'] = df.groupby('Name')['2001:2006'].transform(lambda x: x.cumsum().div(x.sum()))\n    df['cumulative_avg'] = df['cumulative_avg'].replace(0, np.nan)\n    df['cumulative_avg'] = df['cumulative_avg'].fillna(method='ffill')\n    return df\n",
        "\nresult = df.groupby('Name')['2001:2006'].apply(lambda x: x.cumsum().div(x.shift().fillna(0)))\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07'],\n                   'Close': [1460, 1470, 1480, 1450]})\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 1)\ndf['Label'] = df['Label'].astype(int)\ndf.loc[df['Label'] == 1, 'label'] = 1\ndf.loc[df['Label'] == 0, 'label'] = 0\nprint(df)\n",
        "\nresult['label'] = result['Close'].diff().apply(lambda x: 1 if x > 0 else 0 if x == 0 else -1)\nresult.loc[result.index[0], 'label'] = 1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else 0 if x == 0 else -1)\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\n",
        "\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\n",
        "\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\ndf['arrival_time'] = df['arrival_time'].apply(lambda x: pd.to_datetime(x).strftime('%d-%B-%Y %H:%M:%S'))\ndf['departure_time'] = df['departure_time'].apply(lambda x: pd.to_datetime(x).strftime('%d-%B-%Y %H:%M:%S'))\n",
        "\ndf.groupby(['key1']).apply(lambda x: x[x['key2'] == 'one'].size())\n",
        "\ndf.groupby(['key1']).size()\ndf.groupby(['key1']).size().loc[df.groupby(['key1']).size() == 2]\n",
        "\ndf.groupby(['key1']).apply(lambda x: x[x['key2'].endswith(\"e\")].size())\n",
        "\nmax_result = df.max(axis=0)\nmin_result = df.min(axis=0)\n",
        "\nmode_result = df.mode(axis=0)[0]\nmedian_result = df.median(axis=0)[0]\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\ndf = df[(99 <= df['closing_price'] <= 101)]\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\ndf = df[~(99 <= df['closing_price'] <= 101)]\nprint(df)\n",
        "\ndf1 = df.groupby([\"item\", \"otherstuff\"], as_index=False)[\"diff\"].min()\n",
        "\nimport pandas as pd\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\ndf = pd.DataFrame(data={'SOURCE_NAME': strs})\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\ndf = pd.DataFrame(data={'SOURCE_NAME': strs})\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\nexample_df = pd.DataFrame(data={'SOURCE_NAME': strs})\ndef f(df=example_df):\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n# Fill first 50% of NaN values with '0' and last 50% with '1'\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].apply(lambda x: 0 if x.isnan() and x.isin([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) else x, inplace=True))\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n# Fill the first 30% of NaN values with '0'\ndf.loc[df['Column_x'].isna(), 'Column_x'] = df['Column_x'].apply(lambda x: 0 if x.isna() and x.isin([0, 0.5, 1])[0] < 0.3 else x)\n# Fill the middle 30% of NaN values with '0.5'\ndf.loc[df['Column_x'].isna(), 'Column_x'] = df['Column_x'].apply(lambda x: 0.5 if x.isna() and x.isin([0, 0.5, 1])[0] >= 0.3 and x.isin([0, 0.5, 1])[0] < 0.6 else x)\n# Fill the last with '1'\ndf.loc[df['Column_x'].isna(), 'Column_x'] = df['Column_x'].apply(lambda x: 1 if x.isna() and x.isin([0, 0.5, 1])[0] >= 0.6 else x)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Column_x': [0,0,0,0,1,1,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n# Fill NaN values with \"0\" or \"1\" so that the number of \"0\" is 50%(round down) and the number of \"1\" is 50%(round down)\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].value_counts().sort_index()[::-1].iloc[:4000].values, inplace=True)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nresult = pd.DataFrame(zip(a['one'], b['one']), columns=['one', 'two'])\nprint(result)\n",
        "\nresult = pd.DataFrame(zip(a['one'], b['one'], c['one']), columns=['one', 'two'])\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8],[9, 10]]), columns=['one', 'two'])\nresult = pd.DataFrame(zip(a, b), columns=['one', 'two'])\nresult.fillna(np.nan, inplace=True)\nprint(result)\n",
        "\nresult = df.groupby('username')['views'].apply(lambda x: x.quantile(bins))\n",
        "\ngroups = df.groupby(pd.cut(df.views, bins))\ngroups.username.count()\n",
        "\nresult = df.groupby('username')['views'].apply(lambda x: x.apply(lambda y: y.apply(lambda z: z.apply(lambda a: a.apply(lambda b: b if b >= a else 0))))).reset_index()\nresult.columns = ['username', 'views']\nresult = result.pivot_table(index='username', columns='views', values='count')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep=', ')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep='-')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep=', ')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep=', ')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep='-')\nprint(result)\n",
        "\ndf1['city'] = df1['city'].fillna(df2['city'])\ndf1['district'] = df1['district'].fillna(df2['district'])\ndf1 = df1.dropna()\ndf2 = df2.dropna()\n",
        "\ndf1['date'] = pd.to_datetime(df1['date'])\ndf2['date'] = pd.to_datetime(df2['date'])\ndf1 = df1.sort_values(by=['id', 'date'])\ndf2 = df2.sort_values(by=['id', 'date'])\ndf1 = df1.merge(df2, on='id')\ndf1['date'] = df1['date'].dt.strftime('%d-%b-%Y')\ndf1 = df1.dropna()\ndf2 = df2.dropna()\nresult = pd.concat([df1, df2], axis=0)\n",
        "\ndf1['city'] = df1['city'].fillna(df2['city'])\ndf1['district'] = df1['district'].fillna(df2['district'])\ndf1 = df1.sort_values(by=['id', 'date'])\ndf1 = df1.drop_duplicates(subset=['id', 'city', 'district', 'date'])\nresult = df1\n",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\nresult = pd.merge(C, D, how='outer', on='A')\nresult.loc[result.index == result.index[1], 'B'] = result.loc[result.index == result.index[1], 'B_y']\nprint(result)\n",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B_y'] = result['B_y'].fillna(result['B_x'])\nprint(result)\n",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\nresult = pd.merge(C, D, how='outer', on='A')\nresult['dulplicated'] = result['B_x'] == result['B_y']\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(by=['time', 'amount']).tolist())\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(by=['time', 'amount']).tolist())\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(by=['time', 'amount'], ascending=False).tolist())\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\ndf_concatenated = pd.concat([series], axis=1)\nprint(df_concatenated)\n",
        "\nimport pandas as pd\nimport numpy as np\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\ndf_concatenated = pd.DataFrame(index=series.index, columns=['name'] + list(range(len(series.values[0]))), data=series.values)\nprint(df_concatenated)\n",
        "\ndf.columns[df.columns.str.contains(s, case=False)]\n",
        "\ndf[df['name'].str.contains(s, case=False) & df['name'].str.contains(s, case=False, regex=r'\\b' + s + r'\\b')]\n",
        "\ndf = df.rename(columns={'spike-2': 'spike1', 'hey spke': 'spike2', 'spiked-in': 'spike3'})\n",
        "\ndf['code_0'] = df['codes'].apply(lambda x: x[0] if isinstance(x, list) else x)\ndf['code_1'] = df['codes'].apply(lambda x: x[1] if isinstance(x, list) else x)\ndf['code_2'] = df['codes'].apply(lambda x: x[2] if isinstance(x, list) else x)\n",
        "\ndf['code_1'] = df['codes'].apply(lambda x: x[0] if isinstance(x, list) else x)\ndf['code_2'] = df['codes'].apply(lambda x: x[1] if isinstance(x, list) else x)\ndf['code_3'] = df['codes'].apply(lambda x: x[2] if isinstance(x, list) else x)\n",
        "\ndf['code_1'] = df['codes'].apply(lambda x: x[0] if isinstance(x, list) else x)\ndf['code_2'] = df['codes'].apply(lambda x: x[1] if isinstance(x, list) else x)\ndf['code_3'] = df['codes'].apply(lambda x: x[2] if isinstance(x, list) else x)\n",
        "\nresult = []\nfor row in df['col1']:\n    result.extend(row)\n",
        "\nids = str(df.loc[0:index, 'User IDs'].apply(lambda x: ','.join(str(i) for i in x)))\n",
        "\nids = str(df.loc[0:index, 'User IDs'].values.tolist())\n# [Missing Code]\n",
        "\ndf['Time'] = pd.to_datetime(df['Time'])\ndf['Time'] = df['Time'].dt.minute\ndf.groupby('Time')['Value'].mean()\n",
        "\ndf['Time'] = pd.to_datetime(df['Time'])\ndf['Time'] = df['Time'].dt.minute\ndf.groupby('Time')['Value'].sum()\n",
        "\ndf['RANK'] = data.groupby('ID')['TIME'].rank(ascending=True)\n",
        "\ndf['RANK'] = data.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\ndf['RANK'] = data.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3],\n                    'b': [1,2,3,1,2,3,1,2,3],\n                    'c': range(9)}).set_index(['a', 'b'])\nfilt = pd.Series({1:True, 2:False, 3:True})\nresult = df[filt]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3],\n                    'b': [1,2,3,1,2,3,1,2,3],\n                    'c': range(9)}).set_index(['a', 'b'])\nfilt = pd.Series({1:True, 2:False, 3:True})\nresult = df[filt]\nprint(result)\n",
        "\nresult = df.loc[0, :] == df.loc[8, :]\n",
        "\ndf.where(df.isnull().any(axis=1), df.mean(axis=1), inplace=True)\n",
        "\nresult = df.loc[0, :]\nresult = result.loc[8, :]\nresult = result[result != df.loc[0, :]]\n",
        "\nresult = []\nfor i in range(df.shape[0]):\n    diff = df.iloc[i] != df.iloc[8]\n    diff_idx = diff.nonzero()\n    result.extend(zip(df.columns[diff_idx], df.iloc[8][diff_idx]))\nprint(result)\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\n# [Missing Code]\nresult = df.iloc[0]\n",
        "\nresult = df.iloc[0]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, .03), (.21, .18),(pd.NA, .18)],\n                  columns=['dogs', 'cats'])\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2) if not isinstance(x, pd.NA) else x)\nresult = df\nprint(result)\n",
        "\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\ndf.sort_values(by='time', ascending=True, inplace=True)\n",
        "\ndf.sort_index(by='VIM', ascending=True, inplace=True)\n",
        "\ndf = df[(df.index < '2020-02-17 15:30:00') | (df.index > '2020-02-18 21:59:00')]\n",
        "\ndf['Day of Week'] = df.index.weekday\ndf['Date'] = df['Date'].dt.strftime('%d-%m-%Y %A')\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\n# [Missing Code]\nresult = corr[corr > 0.3]\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\nresult = corr[corr > 0.3]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\ndf.columns[-1] = 'Test'\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\ndf.columns[0] = 'Test'\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'bit1': [0, 1, 1],\n                   'bit2': [0, 1, 0],\n                   'bit3': [1, 0, 1],\n                   'bit4': [1, 0, 1],\n                   'bit5': [0, 1, 1]})\n# Finding the frequent values in each row\ndf['frequent'] = df.apply(lambda x: ''.join(str(int(bit)) for bit in x[['bit1', 'bit2', 'bit3', 'bit4', 'bit5']].values), axis=1)\ndf['freq_count'] = df.apply(lambda x: df[x['frequent']].sum(), axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4]})\n# Finding the frequent values in each row\ndf['frequent'] = df.groupby('bit1')['bit2'].apply(lambda x: x.value_counts().idxmin())\ndf['freq_count'] = df.groupby('bit1')['bit2'].apply(lambda x: x.value_counts().count())\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4],\n                   'bit6': [3, 0, 5]})\n# Finding frequent values in each row\ndf['frequent'] = df.groupby('bit1')['bit2'].apply(lambda x: list(set(x)))\ndf['freq_count'] = df.groupby('bit1')['bit2'].apply(lambda x: len(x))\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nprint(result)\n",
        "\nres = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nres['bar'] = res['bar'].fillna(0)\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\nresult = pd.merge(df_a, df_b, on='EntityNum')\nresult = result.drop('b_col', axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\nresult = pd.merge(df_a, df_b, on='EntityNum')\nresult = result.drop(['a_col'], axis=1)\nprint(result)\n"
    ],
    "Numpy": [
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\nresult = a.shape\nprint(result)\n",
        "\nimport numpy as np\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx = np.nan.remove(x)\nprint(x)\n",
        "\nimport numpy as np\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx[np.isnan(x)] = np.inf\nprint(x)\n",
        "\nimport numpy as np\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nresult = np.nan.delete(x, axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.zeros((len(a), len(a[0])), dtype=int)\nfor i in range(len(a)):\n    b[i, a[i]] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.zeros((len(a), len(a) + 1), dtype=int)\nfor i in range(len(a)):\n    b[i, i] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([-1, 0, 3])\nb = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    b[i, a[i]] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([1.5, -0.4, 1.3])\nb = np.zeros((len(a), len(np.unique(a))))\nfor i in range(len(a)):\n    b[i, np.argmin(a == a[i])] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([[1,0,3], [2,4,1]])\nb = np.zeros((a.shape[0], a.shape[1]), dtype=int)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i,j] = np.where(a[i,j] == np.min(a[i,:]), 1, 0)\n        b[i,j] = np.where(a[i,j] == np.max(a[i,:]), 1, b[i,j])\nprint(b)\n",
        "\nimport numpy as np\na = np.array([1,2,3,4,5])\np = 25\nresult = np.percentile(a, p)\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nncol = 2\nB = np.array(A).reshape(-1, ncol)\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nnrow = 3\nB = np.reshape(A, (nrow, len(A)//nrow))\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = np.reshape(A, (len(A)//ncol, ncol))\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = np.reshape(A, (len(A)//ncol, ncol))\nprint(B)\n",
        "\nimport numpy as np\na = np.array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\nshift = 3\nresult = np.roll(a, shift)\nprint(result)\n",
        "\nresult = np.zeros_like(a)\nfor i in range(len(a)):\n    for j in range(len(a[i])):\n        if j + shift < len(a[i]):\n            result[i,j] = a[i][j + shift]\n        else:\n            result[i,j] = np.nan\n",
        "\nimport numpy as np\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = [-2, 3]\n# Apply shift to each row of the array\nresult = np.apply_along_axis(lambda x: np.roll(x, shift[0], axis=0) if shift[1] > 0 else np.roll(x, -shift[0], axis=0), axis=1, arr=a)\nprint(result)\n",
        "\nimport numpy as np\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = r_old\nprint(r_old, r_new)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmax(a)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.ravel_index(np.min(a), a)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmax(a, axis=0)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmax(a, axis=0)\nprint(result)\n",
        "\nimport numpy as np\nexample_a = np.array([[10,50,30],[60,20,40]])\ndef f(a = example_a):\n    max_val = np.amax(a)\n    max_idx = np.where(a == max_val)[0]\n    result = np.ravel(max_idx)\n    return result\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\n# Find the second largest value in the array\nsecond_largest = np.amax(np.amax(a, axis=0), axis=0)\n# Get the indices of the second largest value\nindices = np.where(a == second_largest)[0]\n# Convert the indices to C order\nindices = indices.reshape(-1, 1)\n# Print the result\nprint(indices)\n",
        "\nimport numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\nz = any(isnan(a), axis=0)\ndelete(a, z, axis = 1)\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\na = np.delete(a, np.where(np.isnan(a), True, False))\nprint(a)\n",
        "\nimport numpy as np\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \nresult = np.array(a)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10, 20, 30, 40, 50],\n       [ 6,  7,  8,  9, 10]])\npermutation = [0, 4, 1, 3, 2]\na = np.transpose(a[np.arange(a.shape[0]), permutation])\nprint(a)\n",
        "\nresult = np.array(a[permutation])\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nresult = np.argmin(a)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nresult = np.argmax(a)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1, 0], [0, 2]])\nresult = np.argmin(a, axis=0)\nprint(result)\n",
        "\nresult = np.sin(np.radians(degree))\n",
        "\nresult = np.cos(np.radians(degree))\n",
        "\nimport numpy as np\nnumber = np.random.randint(0, 360)\nif np.sin(np.radians(number)) > np.sin(np.degrees(number)):\n    result = 1\nelse:\n    result = 0\nprint(result)\n",
        "\nimport numpy as np\nvalue = 1.0\nresult = np.deg2rad(np.arcsin(value))\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.pad(A, length, 'constant', constant_value=0)\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.pad(A, length - len(A), 'constant', constant_value=0)\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(4).reshape(2, 2)\npower = 5\na**power\n",
        "\nimport numpy as np\nexample_a = np.arange(4).reshape(2, 2)\ndef f(a = example_a, power = 5):\n    result = np.power(a, power)\n    return result\n",
        "\nimport numpy as np\nnumerator = 98\ndenominator = 42\nresult = np.math.floor(numerator / denominator)\nprint(result)\n",
        "\nimport numpy as np\ndef f(numerator = 98, denominator = 42):\n    result = np.floor(numerator / denominator)\n    return (result, denominator)\n",
        "\nimport numpy as np\nnumerator = 98\ndenominator = 42\nresult = np.divide(numerator, denominator)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = np.mean(np.hstack((a, b, c)), axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = np.amax(np.array([a, b, c]))\nprint(result)\n",
        "\nresult = a[np.diag_indices(5)[::-1]]\n",
        "\nresult = a[np.diag_indices(a.shape[0], k=1)]\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\n# Get the diagonal indices starting from the top right\ndiagonal = np.diag_indices(a.shape[0], k=1)\n# Print the result\nprint(a[diagonal])\n",
        "\n# Get the diagonal indices starting from the bottom left\ndiagonal = np.diag_indices(a.shape[0], k=1)\n",
        "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\nprint(result)\n",
        "\nimport numpy as np\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i,j])\n    return result\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\n",
        "\nresult = np.array([int(digit) for digit in myststr])\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\ncol = 2\nmultiply_number = 5.2\nresult = np.sum(a[:, col] * multiply_number, axis=0)\nprint(result)\n",
        "\nresult = a[row] * multiply_number\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\ndivide_number = 5.2\nresult = a[row] / divide_number\nresult = np.sum(result)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\n# Find the determinant of the matrix\ndet = np.linalg.det(a)\n# If the determinant is 0, the matrix is singular and has no linearly independent vectors\nif det == 0:\n    print(\"The matrix is singular and has no linearly independent vectors.\")\nelse:\n    # Find the eigenvalues of the matrix\n    eigenvalues, eigenvectors = np.linalg.eig(a)\n    # Sort the eigenvalues in descending order\n    eigenvalues = eigenvalues[np.argsort(eigenvalues)[::-1]]\n    # Select the eigenvector with the largest eigenvalue as the first vector\n    first_vector = eigenvectors[:, np.argmax(eigenvalues)]\n    # Initialize the result list\n    result = []\n    # Add the first vector to the result list\n    result.append(first_vector)\n    # Find the remaining eigenvectors that are linearly independent of the first vector\n    remaining_eigenvectors = np.array(eigenvectors[:, np.argpartition(np.linalg.norm(eigenvectors[:, np.argmax(eigenvalues)], axis=1), 2, kth=True)])\n    # Add the remaining eigenvectors to the result list\n    while len(remaining_eigenvectors) > 0:\n        # Find the eigenvector with the largest eigenvalue among the remaining eigenvectors\n        max_eigenvalue_index = np.argmax(np.linalg.norm(remaining_eigenvectors, axis=1))\n        # Add the eigenvector with the largest eigenvalue to the result list\n        result.append(remaining_eigenvectors[max_eigenvalue_index])\n        # Remove the eigenvector with the largest eigenvalue from the remaining eigenvectors\n        remaining_eigenvectors = np.delete(remaining_eigenvectors, max_eigenvalue_index, axis=0)\n    # Print the result\n    print(result)\n",
        "\nimport numpy as np\na = np.random.rand(np.random.randint(5, 10), np.random.randint(6, 10))\nfor i in range(a.shape[0]):\n    result = a[i].size\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.stats\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n# Calculate the sample means and standard deviations\na_mean = np.mean(a)\na_std = np.std(a)\nb_mean = np.mean(b)\nb_std = np.std(b)\n# Calculate the t-statistic\nt_statistic = (a_mean - b_mean) / np.sqrt(np.square(a_std / np.sqrt(a.size)) + np.square(b_std / np.sqrt(b.size)))\n# Calculate the degrees of freedom\ndf = a.size + b.size - 2\n# Calculate the p-value using the t-distribution\np_value = scipy.stats.ttest_ind(a, b, equal_var=False, alpha=0.05)\n# Print the p-value\nprint(p_value)\n",
        "\nimport numpy as np\nimport scipy.stats\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n# Remove nans\na = np.nan.remove(a)\nb = np.nan.remove(b)\n# [Missing Code]\n# Calculate the t-statistic\nt_statistic = scipy.stats.ttest_ind(a, b)\n# [Missing Code]\n# Calculate the p-value\np_value = scipy.stats.ttest_ind(a, b)[1]\nprint(p_value)\n",
        "\nimport numpy as np\nimport scipy.stats\namean = -0.0896\navar = 0.954\nanobs = 40\nbmean = 0.719\nbvar = 11.87\nbnobs = 50\n# Calculate the t-statistic\nt_statistic = (bmean - amean) / np.sqrt((bvar / bnobs) + (avar / anobs))\n# Calculate the degrees of freedom\ndf = bnobs - 1 + anobs - 1\n# Calculate the p-value using the t-distribution\np_value = 2 * scipy.stats.ttest_ind(df, t_statistic)\nprint(p_value)\n",
        "\noutput = np.delete(A, np.where(np.all(A == B, axis=1), axis=0))\n",
        "\noutput = []\nfor i in range(len(A)):\n    for j in range(len(B)):\n        if A[i] not in B[j]:\n            output.append(A[i])\nprint(output)\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nimport numpy as np\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\nresult = np.argsort(np.sum(a, axis=0))\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(12).reshape(3, 4)\na = np.delete(a, 2, axis=1)\nprint(a)\n",
        "\nimport numpy as np\na = np.arange(12).reshape(3, 4)\na = np.delete(a, 2, axis=0)\nprint(a)\n",
        "\nimport numpy as np\na = np.arange(12).reshape(3, 4)\na = a[:, 1:3]\nprint(a)\n",
        "\nimport numpy as np\na = np.arange(12).reshape(3, 4)\ndel_col = np.array([1, 2, 4, 5])\nresult = np.delete(a, del_col, axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.asarray([1,2,3,4])\npos = 2\nelement = 66\na[pos] = element\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\npos = 1\nelement = [3,5]\na = np.insert(a, pos, element, axis=0)\nprint(a)\n",
        "\nimport numpy as np\nexample_a = np.asarray([1,2,3,4])\ndef f(a = example_a, pos=2, element = 66):\n    a = np.insert(a, pos, element)\n    return a\n",
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\npos = [1, 2]\nelement = np.array([[3, 5], [6, 6]])\na = np.insert(a, pos, element, axis=0)\nprint(a)\n",
        "\nresult = []\nfor i in range(len(pairs)):\n    a, b = pairs[i]\n    arr = np.arange(a*b).reshape(a,b)\n    result.append(arr)\nprint(result)\n",
        "\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\nresult = np.all(np.equal(a, a[0]), axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(-1, 1), 3, axis = 1)\nresult = np.all(np.equal(a, a[:, np.newaxis]), axis=1)\nprint(result)\n",
        "\nimport numpy as np\nexample_a = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\ndef f(a = example_a):\n    result = np.all(a == a[0])\n    return result\n",
        "\nimport numpy as np\nx = np.linspace(0, 1, 20)\ny = np.linspace(0, 1, 30)\nresult = np.sum(np.array([[np.cos(x[i])**4 + np.sin(y[j])**2 for j in range(30)] for i in range(20)]))\n",
        "\nimport numpy as np\nexample_x = np.linspace(0, 1, 20)\nexample_y = np.linspace(0, 1, 30)\ndef f(x = example_x, y = example_y):\n    result = np.sum(np.power(np.cos(x), 4) + np.power(np.sin(y), 2))\n    return result\n",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\n# [Missing Code]\nresult = np.ecdf(grades)\n",
        "\nresult = np.apply_along_axis(ecdf, axis=1, arr=grades)\n",
        "\n# Compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high).\nlow, high = 0, 1\nfor i in range(len(grades)):\n    if np.ecdf(grades[i]) < threshold:\n        low = i\n        break\nfor j in range(low+1, len(grades)):\n    if np.ecdf(grades[j]) >= threshold:\n        high = j-1\n        break\n",
        "\nnums = np.random.randint(2, size=size)\nnums[np.random.randint(0, size, size) < one_ratio] = 1\nnums[np.random.randint(0, size, size) >= one_ratio] = 0\n",
        "\nimport torch\nimport numpy as np\na = torch.ones(5)\na_np = np.array(a)\nprint(a_np)\n",
        "\nimport torch\nimport numpy as np\na = np.ones(5)\na_pt = torch.from_numpy(a)\nprint(a_pt)\n",
        "\na_np = np.array(a)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a)\nprint(a_tf)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nresult = np.argsort(a)[::-1]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nresult = np.argsort(a)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nN = 3\nresult = np.argsort(a)[::-1][:N]\nprint(result)\n",
        "\nimport numpy as np\nA = np.arange(16).reshape(4, 4)\nn = 5\nresult = np.power(A, n)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n# Initialize an empty list to store the patches\npatches = []\n# Loop through each element in the array\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        # Create a 2x2 patch around the current element\n        patch = a[i:i+2, j:j+2]\n        # Append the patch to the list\n        patches.append(patch)\n# Convert the list to a 3-d array\nresult = np.array(patches)\nprint(result)\n",
        "\nresult = np.array(np.array_split(a, 2, axis=1)).T\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n# Initialize an empty list to store the patches\npatches = []\n# Loop through each element in the array\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        # Create a 2x2 patch around the current element\n        patch = a[i:i+2, j:j+2]\n        # Check if the patch has already been added to the list\n        if patch not in patches:\n            patches.append(patch)\n# Convert the list to a 3-d array\nresult = np.array(patches)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n# Initialize an empty list to store the patches\npatches = []\n# Loop through the rows of the array\nfor i in range(a.shape[0]):\n    # Loop through the columns of the array\n    for j in range(a.shape[1]):\n        # Create a patch of size 2 by 2\n        patch = a[i:i+2, j:j+2]\n        # Check if the patch is not empty\n        if patch.shape[0] == patch.shape[1] == patch_size:\n            # Append the patch to the list\n            patches.append(patch)\n# Convert the list to a numpy array\nresult = np.array(patches)\nprint(result)\n",
        "\nresult = np.reshape(a, (h, w))\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n# Initialize an empty list to store the patches\npatches = []\n# Loop through the rows of the array\nfor i in range(a.shape[0]):\n    # Loop through the columns of the array\n    for j in range(a.shape[1]):\n        # Create a patch of size 2 by 2\n        patch = a[i:i+2, j:j+2]\n        # Check if the patch is not empty\n        if patch.shape[0] == patch.shape[1] == patch_size:\n            # Append the patch to the list\n            patches.append(patch)\n# Convert the list to a numpy array\nresult = np.array(patches)\nprint(result)\n",
        "\nresult = a[:, low:high+1]\n",
        "\nresult = a[low:high+1]\n",
        "\nresult = a[:, low:high+1]\n",
        "\nimport numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.fromstring(string, dtype=float, sep=',')\nprint(a)\n",
        "\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\nresult = np.random.uniform(min, max, n) / np.log(max / min)\nprint(result)\n",
        "\nimport numpy as np\nmin = 0\nmax = 1\nn = 10000\nresult = np.random.uniform(min, max, n) / np.exp(result)\nprint(result)\n",
        "\nimport numpy as np\ndef f(min=1, max=np.e, n=10000):\n    result = np.random.uniform(min, max, n)\n    result = np.log(result)\n    return result\n",
        "\nB = np.zeros(len(A))\nB[0] = a*A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nimport numpy as np\nimport pandas as pd\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nc = 4\n# Begin of Missing Code\nB = np.zeros(len(A))\nB[0] = a*A[0]\nB[1] = a*A[1]+b*B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n# End of Missing Code\nprint(B)\n",
        "\nimport numpy as np\nresult = np.empty((0,))\nprint(result)\n",
        "\nimport numpy as np\nresult = np.zeros((3,0))\nprint(result)\n",
        "\nimport numpy as np\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = np.sum(np.r_[index])\nprint(result)\n",
        "\nimport numpy as np\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = np.unravel_index(index, dims)[0]\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nindex = ['x', 'y']\ncolumns = ['a','b','c']\ndf = pd.DataFrame(data=np.zeros((2,3), dtype='int32,float32'), index=index, columns=columns)\nprint(df)\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = np.sum(np.bincount(accmap, a), axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = np.maximum(a[index == 0], a[index == 1])\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = np.accumulate(a, accmap)\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])\nprint(result)\n",
        "\nz = np.apply_along_axis(elementwise_function, axis=1, arr=np.column_stack((x, y)))\n",
        "\nimport numpy as np\nprobabilit = [0.333, 0.334, 0.333]\nlista_elegir = [(3, 3), (3, 4), (3, 5)]\nsamples = 1000\nresult = np.random.choice(lista_elegir, samples, probabilit)\nprint(result)\n",
        "\nimport numpy as np\na = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\nresult = a[low_index:high_index, low_index:high_index]\nresult = np.pad(result, ((0, 0), (0, 0)), mode='constant', constant_value=0)\nprint(result)\n",
        "\nresult = x[x >= 0]\n",
        "\nresult = np.array([-2+1j, 2.2+2j])\n",
        "\nbin_data = np.array_split(data, bin_size, axis=0)\nbin_data_mean = np.mean(bin_data, axis=0)\n",
        "\nbin_data = np.array_split(data, bin_size, axis=0)\nbin_data_max = np.amax(bin_data, axis=0)\n",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\n# Begin of Missing Code\nbin_data = np.array_split(data, bin_size, axis=0)\nbin_data_mean = np.mean(bin_data, axis=0)\n# End of Missing Code\nprint(bin_data_mean)\n",
        "\nbin_data = np.array_split(data, bin_size, axis=0)[::-1]\n",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\n# Begin of Missing Code\nbin_data = np.array(data[::-1]).reshape(-1, bin_size)\n# End of Missing Code\nprint(bin_data_mean)\n",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\n# Begin of Missing Code\nbin_data = []\nfor i in range(len(data)):\n    row = data[i]\n    row_start = max(0, i - bin_size // 2)\n    row_end = min(i + bin_size // 2 + 1, len(row))\n    row_data = row[row_start:row_end]\n    bin_data.append(row_data)\n# End of Missing Code\nbin_data_mean = np.mean(bin_data, axis=0)\nprint(bin_data_mean)\n",
        "\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\ndef smoothclamp(x):\n    return 3*x**2 - 2*x**3 if x < x_min else x_max if x > x_max else x\nresult = smoothclamp(x)\n",
        "\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\nN = 5\ndef smoothclamp(x, N=5):\n    return np.power(np.abs(x - x_min) / (x_max - x_min), 1 / N)\n",
        "\nresult = np.correlate(a, b, mode='full')\nresult = result[1:len(result)-1]\nresult = result[result > 0]\n",
        "\nresult = df.values.reshape((4, 15, 5))\n",
        "\nresult = df.values.reshape((15, 4, 5))\n",
        "\nimport numpy as np\na = np.array([1, 2, 3, 4, 5])\nm = 8\nresult = np.zeros((len(a), m), dtype=int)\nfor i in range(len(a)):\n    result[i, :] = np.unpackbits(np.uint8(a[i]), width=m)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = np.zeros((len(a), m), dtype=int)\nfor i in range(len(a)):\n    num = a[i]\n    bits = np.uint8(num)\n    result[i, :m] = bits.reshape(1, m)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = np.zeros((len(a), m), dtype=int)\nfor i in range(len(a)):\n    row = np.unpackbits(np.uint8(a[i]), width=m)\n    result[i] = np.bitwise_xor(row)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n# Calculate the 3rd standard deviation\nresult = np.array(a).std(ddof=1) * 3\n# Print the result\nprint(result)\n",
        "\nimport numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n# Calculate the 2nd standard deviation\nresult = np.array(a).std(ddof=1) * 2\n# Print the result\nprint(result)\n",
        "\nimport numpy as np\nexample_a = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\ndef f(a = example_a):\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    third_std_dev = std_dev * 3\n    start = mean - third_std_dev\n    end = mean + third_std_dev\n    return (start, end)\n",
        "\nimport numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n# Calculate 2nd standard deviation\nsigma = np.std(a, ddof=1)\n# Calculate 2nd standard deviation interval\nlower_bound = a - 2 * sigma\nupper_bound = a + 2 * sigma\n# Create a boolean array indicating outliers\nresult = np.where((a < lower_bound) | (a > upper_bound), True, False)\n",
        "\n# Convert the masked array to a regular array\nmasked_data = np.array(masked_data)\n",
        "\nimport numpy as np\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = 0\nzero_cols = 0\na[zero_rows:zero_rows+1, zero_cols:zero_cols+1] = 0\nprint(a)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\nimport numpy as np\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\na[1:, :-1] = 0\na[:, 1:] = 0\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.where(np.amax(a, axis=1) == a[:, np.newaxis, 0], True, False)\nprint(mask)\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.array([a[:, np.argmin(a, axis=1)] == a[:, 0]])\nprint(mask)\n",
        "\nimport numpy as np\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n# Create a list of ranges\nranges = []\nfor i in range(len(distance)):\n    ranges.append(np.arange(distance[i], distance[i+1]))\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(post, ranges)[0, 1]\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = np.stack([X[:, i].dot(X[:, i].T) for i in range(X.shape[1])], axis=2)\nprint(result)\n",
        "\nX = np.array([[[Y[i,j,k] for j in range(M)] for k in range(M)] for i in range(N)])\n",
        "\nimport numpy as np\na = np.array([9, 2, 7, 0])\nnumber = 0\nis_contained = np.any(a == number)\nprint(is_contained)\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = np.array(list(set(A) - set(B)))\nprint(C)\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = np.intersect(A,B)\nprint(C)\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,4,8])\nC = np.array([A[A >= B[0] & A <= B[1]]])\nprint(C)\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\nresult = rankdata(a)[::-1]\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\nresult = rankdata(a, ascending=False).astype(int)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\nexample_a = [1,2,3,4,3,2,3,4]\ndef f(a = example_a):\n    result = rankdata(a, ascending=False)\n    return result\n",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ny_dists = np.array([[ 0, 1, -2],\n                 [ -1,  0, 1],\n                 [ -2,  1,  0]])\ndists = np.dstack((x_dists, y_dists))\nprint(dists)\n",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ndists = np.dstack((x_dists, y_dists))\nprint(dists)\n",
        "\nresult = a[second][third]\nprint(result)\n",
        "\nimport numpy as np\narr = np.zeros((20, 10, 10, 2), dtype=int)\nprint(arr)\n",
        "\nresult = X.apply(lambda row: row / np.linalg.norm(row, ord=1), axis=1)\n",
        "\nfrom numpy import linalg as LA\nimport numpy as np\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\nresult = np.linalg.norm(X, axis=1)\nprint(result)\n",
        "\nresult = np.array([LA.norm(v,ord=np.inf) for v in X])\n",
        "\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 'foo', 'bar']})\ntarget = 'f'\nchoices = ['XX']\nresult = np.select(df['a'].str.contains(target), choices, default=np.nan)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\n# Calculate distances between all points\ndistances = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    for j in range(i+1, len(a)):\n        distances[i,j] = np.linalg.norm(a[i] - a[j])\n        distances[j,i] = distances[i,j]\nprint(distances)\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n# Calculate distances between all points\ndistances = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        distances[i,j] = np.linalg.norm(a[i] - a[j])\n        distances[j,i] = distances[i,j]\nprint(distances)\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n# Calculate distances between all points\ndistances = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        distances[i,j] = np.linalg.norm(a[i] - a[j])\n# Fill upper triangle matrix\nresult = np.triu(distances)\nprint(result)\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nA = [np.inf, 33.33, 33.33, 33.37]\n",
        "\nresult = np.unique(a)\nresult = result[result != 0]\n",
        "\nimport numpy as np\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3]).reshape(-1, 1)\nresult = np.unique(a)\nprint(result)\n",
        "\ndf = pd.DataFrame(zip(lat, lon, val), columns=['lat', 'lon', 'val'])\n",
        "\ndf = pd.DataFrame(zip(lat, lon, val), columns=['lat', 'lon', 'val'])\n",
        "\nimport numpy as np\nimport pandas as pd\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\ndf = pd.DataFrame(np.column_stack((lat, lon, val)), columns=['lat', 'lon', 'val'])\ndf['maximum'] = df.apply(lambda x: max(x), axis=1)\nprint(df)\n",
        "\nimport numpy as np\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\n# Initialize result as empty list\nresult = []\n# Loop through each row of the grid\nfor i in range(size[0]):\n    # Initialize current row as empty list\n    current_row = []\n    \n    # Loop through each column of the grid\n    for j in range(size[1]):\n        # Get current window\n        window = a[i:i+size[0], j:j+size[1]]\n        \n        # Check if window is entirely within the grid\n        if (i < size[0] - 1) and (j < size[1] - 1):\n            # If window is entirely within the grid, return the full window\n            current_row.append(window)\n        else:\n            # If window is not entirely within the grid, return only the portion of the window that overlaps the grid\n            current_row.append(window[:min(size[0] - i, size[1] - j)])\n    \n    # Append current row to result\n    result.append(current_row)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\n# Initialize result as empty list\nresult = []\n# Loop through each row of the grid\nfor i in range(size[0]):\n    # Initialize current row as empty list\n    current_row = []\n    \n    # Loop through each column of the grid\n    for j in range(size[1]):\n        # Get current window\n        window = a[i:i+size[0], j:j+size[1]]\n        \n        # Check if window is entirely within the grid\n        if (i < size[0] - 1) and (j < size[1] - 1):\n            # If window is entirely within the grid, return the full window\n            current_row.append(window)\n        else:\n            # If window is not entirely within the grid, return only the portion of the window that overlaps the grid\n            current_row.append(window[:min(size[0] - i, size[1] - j)])\n    \n    # Append current row to result\n    result.append(current_row)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1 + 0j, 2 + 0j, np.inf + 0j])\nresult = np.mean(a)\nprint(result)\n",
        "\nimport numpy as np\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    result = np.mean(a)\n    return result\n",
        "\nimport numpy as np\nZ = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\n# Use the slice notation to select the last dimension of Z\nresult = Z[:,:,-1:]\nprint(result)\n",
        "\nimport numpy as np\na = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\nresult = a[-1:, :, :]\nprint(result)\n",
        "\nresult = c in CNTS\n",
        "\nresult = c in CNTS\n",
        "\nresult = intp.griddata(x_new, y_new, a, method='linear')\n",
        "\nimport pandas as pd\nimport numpy as np\ndata = {'D':[2015,2015,2015,2015,2016,2016,2016,2017,2017,2017], 'Q':np.arange(10)}\nname= 'Q_cum'\ndf[name] = np.cumsum(df['D'] == df['D'].shift(1))\nprint(df)\n",
        "\ni = np.diag(np.abs(i))\n",
        "\nimport numpy as np\na = np.array([[1,0,2,3],[0,5,3,4],[2,3,2,10],[3,4, 10, 7]])\na[np.diag(a)] = 0\nprint(a)\n",
        "\nimport numpy as np\nimport pandas as pd\nstart = \"23-FEB-2015 23:09:19.445506\"\nend = \"24-FEB-2015 01:09:22.404973\"\nn = 50\n# Calculate the time step\ntime_step = (end - start) / n\n# Create a numpy array of timesteps\ntimesteps = np.linspace(start, end, n)\n# Convert the numpy array to a pandas datetime index\nresult = pd.to_datetime(timesteps)\nprint(result)\n",
        "\nresult = np.where((x == a) & (y == b), np.arange(len(x)), -1)\n",
        "\nimport numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\nindices = np.where((x == a) & (y == b))[0]\nresult = np.arange(indices[0], indices[-1]+1)\nprint(result)\n",
        "\nimport numpy as np\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\n# Define the function\ndef f(x):\n    return a * x ** 2 + b * x + c\n# Define the cost function\ndef cost_function(a, b, c):\n    return np.sum((f(x) - y) ** 2)\n# Use gradient descent to minimize the cost function\nresult = np.array([0.0, 0.0, 0.0])\nfor i in range(1000):\n    # Compute the gradient of the cost function\n    gradient = -2 * np.dot(x, (f(x) - y)) / len(x)\n    \n    # Update the parameters\n    result += gradient\n    \nprint(result)\n",
        "\nimport numpy as np\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\ndegree = 3\n# Define the function\ndef f(x):\n    return np.poly1d([1, x, x**2])(x)\n# Define the cost function\n# Define the gradient function\ndef gradient_function(params):\n    return -2 * np.poly1d([1, x, x**2])(x) * np.polyval(params, x)\n# Initialize the parameters\nparams = np.array([0, 0, 0])\n# Iterate until convergence\ntolerance = 1e-5\nwhile True:\n    # Compute the gradient\n    grad = gradient_function(params)\n    \n    # Update the parameters\n    params -= learning_rate * grad\n    \n    # Compute the cost\n    cost = cost_function(params)\n    \n    # Check for convergence\n    if abs(cost - previous_cost) < tolerance:\n        break\n    \n    # Update the previous cost\n    previous_cost = cost\nprint(params)\n",
        "\nimport numpy as np\nimport pandas as pd\na = np.arange(4)\ndf = pd.DataFrame(np.repeat([1, 2, 3, 4], 4).reshape(4, -1))\ndf = df.apply(lambda x: x - a)\nprint(df)\n",
        "\nimport numpy as np\nA = np.random.rand(5, 6, 3)\nB = np.random.rand(3, 3)\nresult = np.einsum('ijk,jl->ilk', A, B)\nprint(result)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n",
        "\nresult = MinMaxScaler(arr, feature_range=(0, 1))\n",
        "\nscaler = MinMaxScaler(feature_range=(0, 1))\nresult = scaler.fit_transform(a)\n",
        "\nimport numpy as np\narr = (np.random.rand(100, 50)-0.5) * 50\narr_temp = arr.copy()\nmask = arr_temp < -10\nmask2 = arr_temp < 15\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narry[~mask2] = 30 \nprint(arr)\n",
        "\nimport numpy as np\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\narr_temp = arr.copy()\nmask = arr_temp < n1\nmask2 = arr_temp >= n2\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\nprint(arr)\n",
        "\nimport numpy as np\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\nresult = np.nonzero(s1 != s2)[0].shape[0]\nprint(result)\n",
        "\nimport numpy as np\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\ns1 = np.append(s1, np.nan)\ns2 = np.append(s2, np.nan)\nresult = np.nonzero(s1 != s2)[0].shape[0]\n",
        "\nimport numpy as np\na = [np.array([1,2,3]),np.array([1,2,3]),np.array([1,2,3])]\nresult = np.all(np.array_equal(a, a[0]))\nprint(result)\n",
        "\nimport numpy as np\na = [np.array([np.nan,2,3]),np.array([1,np.nan,3]),np.array([1,2,np.nan])]\nresult = all(np.isnan(arr).all() for arr in a)\nprint(result)\n",
        "\nimport numpy as np\na = np.ones((41, 13))\nshape = (93, 13)\nresult = np.pad(a, shape, 'constant', mode='constant')\nprint(result)\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nresult = np.zeros(shape, dtype=a.dtype)\nresult[:a.shape[0], :a.shape[1]] = a\nprint(result)\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\nresult = np.pad(a, shape, mode='constant', constant_value=element)\nprint(result)\n",
        "\nimport numpy as np\ndef f(arr = np.ones((41, 13)), shape=(93,13)):\n    padded_arr = np.zeros(shape, dtype=arr.dtype)\n    padded_arr[:arr.shape[0], :arr.shape[1]] = arr\n    return padded_arr\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nresult = np.zeros(shape, dtype=a.dtype)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        result[i, j] = a[i, j]\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(12)\n# [Missing Code]\n# a.reshape(a.shape[0]/3,3)\n",
        "\nresult = np.zeros((len(a), len(b[0]), len(b[0][0])))\nfor i in range(len(a)):\n    for j in range(len(b[0])):\n        for k in range(len(b[0][0])):\n            result[i, j, k] = a[i, :, b[0][j, k]]\n",
        "\nresult = np.array(\n  [[a[i, j, 0] for j in range(M)] for i in range(N)]\n)\n",
        "\nresult = np.zeros((len(a), len(b[0]), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b[0])):\n        for k in range(len(b)):\n            result[i, j, k] = a[i, b[0][j], b[1][k]]\n",
        "\nresult = np.sum(a[:, b], axis=2)\n",
        "\nresult = np.sum(a[:, b, :], axis=2)\n",
        "\nimport numpy as np\nimport pandas as pd\ndata = {'a': [1, 2, 3, 4, 5], 'b': [2, -6, 0, -4, 100]}\ndf = pd.DataFrame(data)\nresult = np.where(df['a'] > 1 & df['a'] <= 4, df['b'], np.nan)\nprint(result)\n",
        "\nresult = np.array([[0,1,1,1], [1,1,0,1], [0,0,1,1]])\n",
        "\nimport numpy as np\nA = np.array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 1, 1, 0, 0, 0],\n           [0, 0, 0, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]])\n# Find the indices of non-zero values in the array\nnon_zero_indices = np.where(A != 0)\n# Use these indices to slice the array and get the bounding box\nresult = A[non_zero_indices[0], non_zero_indices[1]]\nprint(result)\n",
        "\nimport numpy as np\nim = np.array([[1,1,1,1,1,5],\n               [1,0,0,1,2,0],\n               [2,1,0,0,1,0],\n               [1,0,0,7,1,0],\n               [1,0,0,0,0,0]])\nresult = np.zeros((len(im), len(im[0])))\nfor i in range(len(im)):\n    for j in range(len(im[0])):\n        if im[i][j] == 0:\n            result[i][j] = 1\nprint(result)\n",
        "\nresult = np.array([[0,5,1,2],\n                   [1,8,0,1],\n                   [0,0,7,1]])\n"
    ],
    "Matplotlib": [
        "\nx = 10 * np.random.randn(10)\ny = x\n# plot x vs y, label them using \"x-y\" in the legend\nsns.scatterplot(x=x, y=y, label=\"x-y\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Scatter plot of x vs y\")\nplt.legend()\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n# Turn on minor ticks on y-axis only\nplt.yaxis().set_yticks(np.arange(0, 1.1, 0.1))\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n# Turn on minor ticks\nplt.xticks(np.arange(len(x)), x, rotation=45)\nplt.yticks(np.arange(len(y)), y)\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n# Turn on minor ticks on x-axis only\nplt.xaxis.set_major_locator(plt.MaxNLocator(4))\nplt.xaxis.set_minor_locator(plt.MinNLocator(1))\n",
        "\nx = np.arange(10)\n# draw a line (with random y) for each different line style\nsns.lineplot(x=x, y=np.random.rand(10), hue=np.random.randint(0, 4, 10), style='dashed')\n",
        "\nx = np.arange(10)\n# draw a line (with random y) for each different line style\nsns.lineplot(x=x, y=np.random.rand(10), hue=np.random.randint(0, 4, 10), style='dashed')\n",
        "\n# Add a title and axis labels\nplt.title('Line Plot with Diamond Marker')\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\n# Add a legend\nplt.legend(['Diamond Marker'])\n",
        "\nx = np.arange(10)\ny = np.random.randn(10)\n# line plot x and y with a thick diamond marker\nplt.plot(x, y, '^', linewidth=2)\n",
        "\n# To set the y axis limit to be 0 to 40, we can use the `set_ylim()` method of the `Axes` object.\nax.set_ylim(0, 40)\n",
        "\nplt.plot(x)\nplt.axvline(x=np.arange(2, 5), color='red')\n",
        "\n# draw a full line from (0,0) to (1,2)\nx = np.linspace(0, 1, 100)\ny = np.linspace(0, 2, 100)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Full Line from (0,0) to (1,2)')\n",
        "\n# draw a line segment from (0,0) to (1,2)\nx = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n",
        "\nsns.scatterplot(x='Height (cm)', y='Weight (kg)', hue='Gender', data=df)\n",
        "\n# create a seaborn scatter plot with hue based on x values\nsns.scatterplot(x=x, y=y, hue=x)\nplt.title('Seaborn Scatter Plot with Hue Based on X Values')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.sin(x)\n# draw a line plot of x vs y using seaborn and pandas\nsns.lineplot(x=x, y=y)\nplt.show()\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n# in plt.plot(x, y), use a plus marker and give it a thickness of 7\nplt.plot(x, y, '+', linewidth=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\n# set legend title to xyz and set the title font to size 20\nplt.legend(title='xyz', fontsize=20)\nplt.title('Cosine Wave', fontsize=20)\n",
        "\n# To set the face color of the markers to have an alpha (transparency) of 0.2, we can use the `set_facecolor` method of the `Marker` class in `matplotlib`.\n# We can access the `Marker` object by using the `get_markers` method of the `Artist` object, which returns a list of all the markers in the plot.\n# We can then loop through the list of markers and set the face color of each marker using the `set_facecolor` method.\n# Here's the code to do that:\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30, edgecolor=\"black\")\n",
        "\nplt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"red\", markerfacecolor=\"red\")\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# rotate the x axis labels clockwise by 45 degrees\nplt.gca().set_xtick_params(rotation=45)\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# rotate the x axis labels counterclockwise by 45 degrees\nplt.xticks(rotation=45)\n",
        "\n# To put x-axis ticklabels at 0, 2, 4, ..., we can use the `set_xticks()` method of the `Axes` object.\n# We can pass a list of tick positions to this method, where each position is an integer multiple of the tick interval (in this case, 1).\n# We can also pass a label for each tick position using the `set_xticklabels()` method.\nplt.xaxis.set_xticks([0, 2, 4, 6, 8, 10])\nplt.xaxis.set_xticklabels(['0', '2\u03c0', '4\u03c0', '6\u03c0', '8\u03c0', '10\u03c0'])\n",
        "\n",
        "\nH = np.random.randn(10, 10)\n# color plot of the 2d array H\nplt.imshow(H, cmap='gray')\nplt.title('Random 2D Array')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n",
        "\n# To display the 2D array H in black and white, we can use the imshow function from matplotlib.pyplot.\n# This function displays an image, and by default, it uses grayscale.\nplt.imshow(H, cmap='gray')\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\n# set xlabel as \"X\"\n# put the x label at the right end of the x axis\nxlabel_pos = len(x) - 1\nplt.xlabel(\"X\", x=xlabel_pos)\n",
        "\ng.set_xlabel(\"Orbital Period (days)\", rotation=90)\n",
        "\nmyTitle = myTitle[:50] + \"\\n\" + myTitle[50:]\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n# make the y axis go upside down\ny = -y\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Upside down y-axis')\nplt.show()\n",
        "\nx = np.random.randn(10)\ny = x\nplt.scatter(x, y)\n# put x ticks at 0 and 1.5 only\nx_ticks = np.arange(0, 1.5, 0.1)\nplt.xticks(x_ticks)\n",
        "\n# To put y ticks at -1 and 1 only, we can use the `set_yticks()` method of the `Axes` object.\nplt.scatter(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.ylim([-1, 1])\nplt.yticks([-1, 1])\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\n# plot x, then y then z, but so that x covers y and y covers z\nfig, axs = plt.subplots(figsize=(10, 5))\naxs[0].plot(x)\naxs[1].plot(y)\naxs[2].plot(z)\naxs[0].set_title('X')\naxs[1].set_title('Y')\naxs[2].set_title('Z')\naxs[1].set_xlim(min(x), max(x))\naxs[2].set_ylim(min(y), max(y))\nplt.show()\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n# in a scatter plot of x, y, make the points have black borders and blue face\nplt.scatter(x, y, c='b', edgecolors='k')\n",
        "\n# To make all axes ticks integers, we can use the `set_xticks()` and `set_yticks()` methods of the `matplotlib.pyplot` module.\n# Set the x-axis ticks to be integers from 0 to 9\nplt.xaxis().set_major_locator(plt.MaxNLocator(9))\n# Set the y-axis ticks to be integers from 0 to 19\nplt.yaxis().set_major_locator(plt.MaxNLocator(19))\n",
        "\nsns.factorplot(y=\"coverage\", x=\"reports\", kind=\"bar\", data=df, label=\"Total\")\nplt.yticks(rotation=45)\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nax = sns.lineplot(x=x, y=y)\n# Plot a dashed line on the seaborn lineplot\nax.plot(x, y, linestyle='--')\n",
        "\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n# plot x vs y1 and x vs y2 in two subplots, sharing the x axis\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax1.plot(x, y1, label='y1')\nax2.plot(x, y2, label='y2')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.legend()\nax2.set_ylabel('y2')\nplt.show()\n",
        "\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n# plot x vs y1 and x vs y2 in two subplots\nfig, ax1 = plt.subplots(figsize=(10, 5))\nax1.plot(x, y1, label='y1')\nax1.plot(x, y2, label='y2')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Sine and Cosine')\nax1.legend()\nax2 = ax1.twinx()\nax2.plot(x, y1, label='y1')\nax2.plot(x, y2, label='y2')\nax2.set_ylabel('y')\nax2.set_title('Sine and Cosine')\nax2.legend()\n# remove the frames from the subplots\nfig.tight_layout()\nplt.show()\n",
        "\nsns.lineplot(x=\"x\", y=\"y\", data=df)\nplt.xlabel(\"\")\n",
        "\nx = np.arange(10)\ny = np.sin(x)\ndf = pd.DataFrame({\"x\": x, \"y\": y})\nsns.lineplot(x=\"x\", y=\"y\", data=df, tick_params={\"label_bottom\": False})\n",
        "\nplt.grid(True, color='black', linestyle='--')\nplt.xticks([3, 4], ['A', 'B'])\n",
        "\n# To add labels to the yticks, use the following code:\nplt.yticks([3, 4], ['Label 3', 'Label 4'])\n",
        "\n",
        "\n",
        "\nx = 10 * np.random.randn(10)\ny = x\nplt.plot(x, y, label=\"x-y\")\n# put legend in the lower right\nplt.legend(handles=[], labels=['x-y'], handlelength=0.5, handlelocation='lower right')\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\n",
        "\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n",
        "\nax.set_xlabel(\"\")\nax.set_xticks([])\nax.set_ylabel(\"\")\nax.set_yticks([])\nax.invert_yaxis()\n",
        "\n",
        "\n",
        "\n# Move the y axis ticks to the right\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y over x')\nplt.grid()\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and label y axis \"Y\"\n# Show y axis ticks on the left and y axis label on the right\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(rotation=45)\nplt.show()\n",
        "\ntips = sns.load_dataset(\"tips\")\n# Make a seaborn joint regression plot (kind='reg') of 'total_bill' and 'tip' in the tips dataframe\n# change the line and scatter plot color to green but keep the distribution plot in blue\nsns.regplot(x='total_bill', y='tip', data=tips, kind='reg', hue='sex', palette=['green', 'blue'])\n",
        "\ntips = sns.load_dataset(\"tips\")\n# Make a seaborn joint regression plot (kind='reg') of 'total_bill' and 'tip' in the tips dataframe\n# change the line color in the regression to green but keep the histograms in blue\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', hue='sex', palette=['blue', 'green'])\n",
        "\ntips = sns.load_dataset(\"tips\")\n# Make a seaborn joint regression plot (kind='reg') of 'total_bill' and 'tip' in the tips dataframe\n# do not use scatterplot for the joint plot\nsns.regplot(x='total_bill', y='tip', data=tips)\n",
        "\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n# For data in df, make a bar plot of s1 and s1 and use celltype as the xlabel\n# Make the x-axis tick labels horizontal\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=45)\nplt.title(\"Bar Plot of s1 and s2\")\nplt.legend()\n",
        "\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n# For data in df, make a bar plot of s1 and s1 and use celltype as the xlabel\n# Make the x-axis tick labels rotate 45 degrees\nfig, ax = plt.subplots()\nax.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nax.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nax.set_xlabel(\"celltype\")\nax.set_xticklabels(df[\"celltype\"], rotation=45)\nax.set_ylabel(\"value\")\nax.legend()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and label the x axis as \"X\"\n# Make both the x axis ticks and the axis label red\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.xticks(rotation=45, color=\"red\")\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and label the x axis as \"X\"\n# Make the line of the x axis red\nplt.plot(x, y, label='y')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.gca().set_xlabel_color('red')\n",
        "\nplt.show()\n",
        "\n# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\nx = [0.22058956, 0.33088437, 2.20589566]\ny = [0, 0, 0]\nplt.plot(x, y, 'k--')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nplt.gca().invert_yaxis()\nplt.gca().xaxis.set_major_formatter(matplotlib.ticker.MaxNLocator(n=len(xlabels)))\nplt.gca().yaxis.set_major_formatter(matplotlib.ticker.MaxNLocator(n=len(ylabels)))\nplt.gca().xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(n=len(xlabels)))\nplt.gca().yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(n=len(ylabels)))\nplt.gca().xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, pos: xlabels[x]))\nplt.gca().yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, pos: ylabels[x]))\n",
        "\n# Add a legend for the third curve in the first subplot\nax.legend(loc=0)\n# Add a legend for the second curve in the second subplot\nax2.legend(loc=0)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make two side-by-side subplots and in each subplot, plot y over x\n# Title each subplot as \"Y\"\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title('Y')\naxs[1].plot(x, y)\naxs[1].set_title('Y')\n",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n# make a seaborn scatter plot of bill_length_mm and bill_depth_mm\n# use markersize 30 for all data points in the scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, hue=\"species\", markersize=30)\n",
        "\na = [2.56422, 3.77284, 3.52623]\nb = [0.15, 0.3, 0.45]\nc = [58, 651, 393]\n# make scatter plot of a over b and annotate each data point with correspond numbers in c\nfig, ax = plt.subplots()\nax.scatter(b, a, c=c)\nax.annotate(str(i) for i, txt in enumerate(c))\nax.set_xlabel('b')\nax.set_ylabel('a')\nax.set_title('Scatter plot of a over b with annotations')\n",
        "\n",
        "\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\n# Make a histogram of x and show outline of each bar in the histogram\n# Make the outline of each bar has a line width of 1.2\nplt.hist(x, bins=10, linewidth=1.2)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Make two subplots. Make the first subplot three times wider than the second subplot but they should have the same height.\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[1].plot(x, y)\naxs[0].set_title('First Subplot')\naxs[1].set_title('Second Subplot')\naxs[0].set_xlabel('X-axis')\naxs[0].set_ylabel('Y-axis')\naxs[1].set_xlabel('X-axis')\naxs[1].set_ylabel('Y-axis')\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nbins = np.linspace(-1, 1, 100)\n# Plot two histograms of x and y on a single chart with matplotlib\n# Set the transparency of the histograms to be 0.5\nplt.hist(x, bins=bins, alpha=0.5, label='x')\nplt.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\n# Plot a grouped histograms of x and y on a single chart with matplotlib\n# Use grouped histograms so that the histograms don't overlap with each other\nplt.hist(x, bins=5, alpha=0.5, label='x')\nplt.hist(y, bins=5, alpha=0.5, label='y')\nplt.legend()\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Grouped Histograms of X and Y')\n",
        "\n",
        "\nx = np.random.random((10, 10))\ny = np.random.random((10, 10))\n# make two colormaps with x and y and put them into different subplots\n# use a single colorbar for these two subplots\nfig, ax1 = plt.subplots(figsize=(10, 10))\ncmap1 = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=0, vmax=1), cmap='viridis')\ncmap1._A = []\ncmap1._B = []\ncmap1._C = []\ncmap1._D = []\ncmap1._E = []\ncmap1._F = []\ncmap1._G = []\ncmap1._H = []\ncmap1._I = []\ncmap1._J = []\ncmap1._K = []\ncmap1._L = []\ncmap1._M = []\ncmap1._N = []\ncmap1._O = []\ncmap1._P = []\ncmap1._Q = []\ncmap1._R = []\ncmap1._S = []\ncmap1._T = []\ncmap1._U = []\ncmap1._V = []\ncmap1._W = []\ncmap1._X = []\ncmap1._Y = []\ncmap1._Z = []\ncmap1._a = []\ncmap1._b = []\ncmap1._c = []\ncmap1._d = []\ncmap1._e = []\ncmap1._f = []\ncmap1._g = []\ncmap1._h = []\ncmap1._i = []\ncmap1._j = []\ncmap1._k = []\ncmap1._l = []\ncmap1._m = []\ncmap1._n = []\ncmap1._o = []\ncmap1._p = []\ncmap1._q = []\ncmap1._r = []\ncmap1._s = []\ncmap1._t = []\ncmap1._u = []\ncmap1._v = []\ncmap1._w = []\ncmap1._x = []\ncmap1._y = []\ncmap1._z = []\ncmap1._A = []\ncmap1._B = []\ncmap1._C = []\ncmap1._D = []\ncmap1._E = []\ncmap1._F = []\ncmap1._G = []\ncmap1._H = []\ncmap1._I = []\ncmap1._J = []\ncmap1._K = []\ncmap1._L = []\ncmap1._M = []\ncmap1._N = []\ncmap1._O = []\ncmap1._P = []\ncmap1._Q = []\ncmap1._R = []\ncmap1._S = []\ncmap1._T = []\ncmap1._U = []\ncmap1._V = []\ncmap1._W = []\ncmap1._X = []\ncmap1._Y = []\ncmap1._Z = []\ncmap1._a = []\ncmap1._b = []\ncmap1._c = []\ncmap1._d = []\ncmap1._e = []\ncmap1._f = []\ncmap1._g = []\ncmap1._h = []\ncmap1._i = []\ncmap1._j = []\ncmap1._k = []\ncmap1._l = []\ncmap1._m = []\ncmap1._n = []\ncmap1._o = []\ncmap1._p = []\ncmap1._q = []\ncmap1._r = []\ncmap1._s = []\ncmap1._t = []\ncmap1._u = []\ncmap1._v = []\ncmap1._w = []\ncmap1._x = []\ncmap1._y = []\ncmap1._z = []\ncmap1._A = []\ncmap1._B = []\ncmap1._C = []\ncmap1._D = []\ncmap1._E = []\ncmap1._F = []",
        "\nx = np.random.random((10, 2))\n# Plot each column in x as an individual line and label them as \"a\" and \"b\"\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.xlabel('Column 1')\nplt.ylabel('Column 2')\nplt.title('Random Data')\nplt.legend()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n# plot y over x and z over a in two different subplots\nfig, ax1 = plt.subplots(figsize=(10, 5))\nax1.plot(x, y, label='y over x')\nax1.plot(z, a, label='z over a')\nax1.set_title('Y and Z')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.legend()\nfig, ax2 = plt.subplots(figsize=(10, 5))\nax2.plot(y, z, label='y over z')\nax2.plot(a, z, label='a over z')\nax2.set_title('Y and Z')\nax2.set_xlabel('Y')\nax2.set_ylabel('Z')\nax2.legend()\nplt.show()\n",
        "\npoints = [(3, 5), (5, 10), (10, 150)]\n# plot a line plot for points in points.\n# Make the y-axis log scale\nplt.plot(points[:, 0], points[:, 1], label='Points')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Points')\nplt.logy()\nplt.legend()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# plot y over x\nplt.plot(x, y)\nplt.title('y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\n",
        "\n",
        "\n",
        "\nx = np.arange(0, 1000, 50)\ny = np.arange(0, 1000, 50)\n# plot y over x on a log-log plot\n# mark the axes with numbers like 1, 10, 100. do not use scientific notation\nplt.loglog(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks([1, 10, 100])\nplt.yticks([1, 10, 100])\nplt.show()\n",
        "\ndf = pd.DataFrame(\n    np.random.randn(50, 4),\n    index=pd.date_range(\"1/1/2000\", periods=50),\n    columns=list(\"ABCD\"),\n)\ndf = df.cumsum()\n# make four line plots of data in the data frame\n# show the data points  on the line plot\nfig, axs = plt.subplots(4, figsize=(10, 15))\ndf.plot(ax=axs[0], kind='line', title='A')\ndf.plot(ax=axs[1], kind='line', title='B')\ndf.plot(ax=axs[2], kind='line', title='C')\ndf.plot(ax=axs[3], kind='line', title='D')\n",
        "\ndata = [1000, 1000, 5000, 3000, 4000, 16000, 2000]\n# Make a histogram of data and renormalize the data to sum up to 1\nplt.hist(data, bins=np.arange(min(data), max(data)+1))\nplt.xlabel('Data')\nplt.ylabel('Frequency')\nplt.title('Histogram of Data')\nplt.xticks(rotation=45)\nplt.ylim([0, 1])\nplt.grid(True)\n# Renormalize the data to sum up to 1\nplt.gca().set_ydata(plt.gca().get_ydata() / plt.gca().get_ydata().sum())\n# Format the y tick labels into percentage and set y tick labels as 10%, 20%, etc.\nplt.gca().set_yticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\nplt.gca().set_yticklabels(['10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
        "\n# To make the marker have a 0.5 transparency, we can set the alpha parameter of the scatter function to 0.5.\n# To keep the lines solid, we can set the color parameter of the plot function to 'k' (black).\n",
        "\nx = np.arange(10)\ny = np.arange(10)\na = np.arange(10)\nz = np.arange(10)\n# Plot y over x and a over z in two side-by-side subplots.\n# Label them \"y\" and \"a\" and make a single figure-level legend using the figlegend function\nfig, ax1 = plt.subplots(figsize=(10, 5))\nax2 = ax1.twinx()\nax1.plot(x, y, label='y')\nax2.plot(z, a, label='a')\nax1.set_title('y over x')\nax2.set_title('a over z')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax2.set_xlabel('z')\nax2.set_ylabel('a')\nplt.legend(loc='best')\n",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n# Make 2 subplots.\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\n# Do not share y axix for the subplots\nfig, ax1 = plt.subplots(figsize=(10, 5))\nax2 = ax1.twinx()\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\nax1.set_title(\"Bill Depth vs Bill Length\")\nax1.set_xlabel(\"Bill Length (mm)\")\nax1.set_ylabel(\"Bill Depth (mm)\")\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\nax2.set_title(\"Flipper Length vs Bill Length\")\nax2.set_xlabel(\"Bill Length (mm)\")\nax2.set_ylabel(\"Flipper Length (mm)\")\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nfig, ax = plt.subplots(1, 1)\nplt.xlim(1, 10)\nplt.xticks(range(1, 10))\nax.plot(y, x)\n# change the second x axis tick label to \"second\" but keep other labels in numerical\nax.set_xticklabels([\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\"])\n",
        "\n",
        "\nplt.xticks(plt.xticks() + [2.1, 3, 7.6], rotation=45)\n",
        "\n# To rotate the xticklabels to -60 degree and set the xticks horizontal alignment to left, we can use the plt.xticks() function.\n# The rotation parameter is set to 60 to rotate the xticklabels to -60 degree.\n# The ha parameter is set to 'left' to set the xticks horizontal alignment to left.\n",
        "\nx = np.arange(2010, 2020)\ny = np.arange(10)\nplt.plot(x, y)\n# Rotate the yticklabels to -60 degree. Set the xticks vertical alignment to top.\nplt.xticks(rotation=90)\nplt.yticks(rotation=-60)\nplt.gca().invert_yaxis()\n",
        "\nx = np.arange(2010, 2020)\ny = np.arange(10)\nplt.plot(x, y)\n# Set the transparency of xtick labels to be 0.5\nxtick = plt.gca().get_xtick()\nxtick.set_alpha(0.5)\n",
        "\nplt.gca().set_xticks(np.arange(10))\nplt.gca().set_yticks(np.arange(10))\nplt.gca().set_xticklabels(np.arange(10))\nplt.gca().set_yticklabels(np.arange(10))\nplt.gca().set_xtickparams(top=True, bottom=False, labeltop=True, labelbottom=False)\nplt.gca().set_ytickparams(top=False, bottom=True, labeltop=False, labelbottom=True)\nplt.gca().set_xtickmargin(0.1)\nplt.gca().set_ytickmargin(0.1)\n",
        "\nplt.gca().set_yticks(np.arange(0, 11, 1))\nplt.gca().set_yticklabels(np.arange(0, 11, 1))\nplt.gca().set_xticks(np.arange(0, 11, 1))\nplt.gca().set_xticklabels(np.arange(0, 11, 1))\nplt.gca().set_ylabel('y')\nplt.gca().set_xlabel('x')\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make a two columns and one row subplots. Plot y over x in each subplot.\n# Give the plot a global title \"Figure\"\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[1].plot(x, y)\nplt.title(\"Figure\")\nplt.show()\n",
        "\n",
        "\n# Add a title and axis labels\nplt.title('Scatter Plot with Vertical Line Hatch')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\n",
        "\n# To remove the edge of the marker, we can set the edgecolor to black and linewidth to 0.5.\n# To use vertical line hatch for the marker, we can set the hatch parameter to 'v'.\n# To set the size of the marker, we can set the s parameter to 50.\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Make a scatter plot with x and y\n# Use star hatch for the marker\nplt.scatter(x, y, hatch='*')\n# Add labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Scatter Plot with Star Hatch')\n# Show the plot\nplt.show()\n",
        "\n# To solve the problem, we need to add a title and axis labels to the plot\nplt.title('Scatter Plot with Star Hatch and Vertical Line Hatch')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\n",
        "\n",
        "\nx = np.linspace(0.1, 2 * np.pi, 41)\ny = np.exp(np.sin(x))\n# make a stem plot of y over x and set the orientation to be horizontal\nplt.stem(x, y, orientation='horizontal')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Stem Plot of y over x')\nplt.show()\n",
        "\n# Add title and axis labels\nplt.title(\"Bar Plot of Data\")\nplt.xlabel(\"X-Axis\")\nplt.ylabel(\"Y-Axis\")\n",
        "\n# Make a solid vertical line at x=3 and label it \"cutoff\". Show legend of this plot.\nx = [1, 2, 3, 4, 5]\ny = [1, 2, 3, 4, 5]\nplt.plot(x, y)\nplt.axvline(x=3, color='r', linestyle='--')\nplt.annotate('cutoff', xy=(3, 3.5), xytext=(3.5, 3.5), arrowprops=dict(facecolor='black', shrink=0.05))\nplt.legend()\nplt.show()\n",
        "\nlabels = [\"a\", \"b\"]\nheight = [3, 4]\n# Use polar projection for the figure and make a bar plot with labels in `labels` and bar height in `height`\nfig, ax = plt.subplots(figsize=(10, 10), projection='polar')\nax.bar(theta=0, height=height, data=labels, align='center')\nax.set_theta_direction(-1)\nax.set_theta_offset(90)\nax.set_rlim(0, 10)\nax.set_rticks([])\nax.set_rlabel('')\nplt.show()\n",
        "\nl = [\"a\", \"b\", \"c\"]\ndata = [225, 90, 50]\n# Make a donut plot of using `data` and use `l` for the pie labels\n# Set the wedge width to be 0.4\nplt.pie(data, labels=l, autopct='%1.1f%%', startangle=90, shadow=True, wedgewidth=0.4)\nplt.axis('equal')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\nplt.plot(x, y, 'b--')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y over x with blue dashed grid lines')\nplt.grid(True, color='b', linestyle='--')\nplt.show()\n",
        "\n",
        "\n# Bold the pie labels\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, fontweight='bold')\n",
        "\n# Bold the pie labels\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, fontweight='bold')\n",
        "\n# To make the marker transparent, we can set the alpha parameter to 0.5.\n# To make the edge color non-transparent, we can set the edgecolor parameter to 'black'.\n",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\nsns.distplot(df[\"bill_length_mm\"], color=\"blue\")\n# Plot a vertical line at 55 with green color\nplt.axvline(x=55, color=\"green\", linestyle=\"--\")\n",
        "\n# Specify the values of blue bars (height)\nblue_bar = (23, 25, 17)\n# Specify the values of orange bars (height)\norange_bar = (19, 18, 14)\n# Plot the blue bar and the orange bar side-by-side in the same bar plot.\n# Make  sure the bars don't overlap with each other.\nfig, ax = plt.subplots()\nax.bar(range(len(blue_bar)), blue_bar, color='blue')\nax.bar(range(len(blue_bar)), orange_bar, color='orange', bottom=blue_bar)\nax.set_xticks(range(len(blue_bar)))\nax.set_xticklabels(['A', 'B', 'C'])\nax.set_ylabel('Height')\nplt.show()\n",
        "\n# Add a title to the figure\nplt.title('Two Line Charts')\n",
        "\nx = np.arange(10)\ny = np.linspace(0, 1, 10)\n# Plot y over x with a scatter plot\n# Use the \"Spectral\" colormap and color each data point based on the y-value\nplt.scatter(x, y, c=y, cmap='Spectral')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y over x')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# plot y over x\n# use a tick interval of 1 on the a-axis\nplt.plot(x, y)\nplt.xticks(np.arange(10), np.arange(10))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y over x')\nplt.show()\n",
        "\ndf = sns.load_dataset(\"penguins\")[[\"bill_length_mm\", \"species\", \"sex\"]]\n# Use seaborn factorpot to plot multiple barplots of \"bill_length_mm\" over \"sex\" and separate into different subplot columns by \"species\"\n# Do not share y axis across subplots\nsns.factorplot(x=\"species\", y=\"sex\", data=df, hue=\"bill_length_mm\")\n",
        "\n# draw a circle centered at (0.5, 0.5) with radius 0.2\nplt.circle((0.5, 0.5), 0.2, color='red')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and use the greek letter phi for title. Bold the title and make sure phi is bold.\nplt.plot(x, y)\nplt.title('$\\phi$', fontweight='bold')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\n",
        "\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(x, y, label=\"Line\")\nplt.plot(y, x, label=\"Flipped\")\n# Add a two column legend to the plot\nplt.legend(loc='upper right', handle_length=0.8)\n# Show the plot\nplt.show()\n",
        "\n# To add a legend to the plot, we can use the `legend()` function.\n# To add two markers on the line, we can use the `axvline()` function.\n# Add a legend\nplt.legend()\n# Add two markers on the line\nplt.axvline(x=5, y=5, color='red', marker='o')\nplt.axvline(x=5, y=10, color='blue', marker='x')\n",
        "\ndata = np.random.random((10, 10))\n# plot the 2d matrix data with a colorbar\nplt.imshow(data, cmap='gray')\nplt.colorbar()\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x. Give the plot a title \"Figure 1\". bold the word \"Figure\" in the title but do not bold \"1\"\nplt.plot(x, y)\nplt.title('Figure 1')\nplt.title.set_fontweight('bold')\nplt.show()\n",
        "\ndf = pd.DataFrame(\n    {\n        \"id\": [\"1\", \"2\", \"1\", \"2\", \"2\"],\n        \"x\": [123, 22, 356, 412, 54],\n        \"y\": [120, 12, 35, 41, 45],\n    }\n)\n# Use seaborn to make a pairplot of data in `df` using `x` for x_vars, `y` for y_vars, and `id` for hue\n# Hide the legend in the output figure\nsns.pairplot(df, x_vars=[\"x\", \"y\"], y_vars=[\"id\"], hue=\"id\", legend=False)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and invert the x axis\nplt.plot(y, x)\nplt.xlabel('Inverted X-axis')\nplt.ylabel('Y-axis')\nplt.title('Plot of Y over Inverted X-axis')\nplt.show()\n",
        "\nx = np.arange(11)\ny = np.arange(11)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\n# Plot a scatter plot x over y and set both the x limit and y limit to be between 0 and 10\n# Turn off axis clipping so data points can go beyond the axes\nplt.scatter(x, y, s=50)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n",
        "\n# Set the title of the plot\nplt.title('Scatter Plot with Red Inside and Black Border')\n# Set the x-axis label\nplt.xlabel('X-axis')\n# Set the y-axis label\nplt.ylabel('Y-axis')\n# Show the plot\nplt.show()\n",
        "\n# To solve the problem, we need to plot the same data (y over x) on a 2 by 2 subplots with a figure size of (15, 15). We can do this by creating a figure with 4 subplots using the subplots() function from matplotlib.pyplot. We can then use a for loop to iterate through each subplot and plot the data using the plot() function.\n",
        "\nplt.hist(x, bins=5, range=(0, 10), alpha=0.5, linewidth=2)\n",
        "\n",
        "\nxvec = np.linspace(-5.0, 5.0, 100)\nx, y = np.meshgrid(xvec, xvec)\nz = -np.hypot(x, y)\nplt.contourf(x, y, z)\n# draw x=0 and y=0 axis in my contour plot with white color\nplt.axvline(0, color='white')\nplt.axhline(0, color='white')\n",
        "\nax.errorbar(box_position, box_height, yerr=box_errors, color=c)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n# Plot y over x and z over a in two side-by-side subplots\nfig, ax1 = plt.subplots(figsize=(10, 5))\nax2 = ax1.twinx()\n# Make \"Y\" the title of the first subplot and \"Z\" the title of the second subplot\nax1.set_title('Y')\nax2.set_title('Z')\n# Raise the title of the second subplot to be higher than the first one\nax2.set_ylabel('Z')\nax1.set_ylabel('Y')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make 4 by 4 subplots with a figure size (5,5)\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\n# in each subplot, plot y over x and show axis tick labels\nfor i in range(16):\n    axs[i // 4, i % 4].plot(x, y)\n    axs[i // 4, i % 4].set_xlabel('x')\n    axs[i // 4, i % 4].set_ylabel('y')\n# give enough spacing between subplots so the tick labels don't overlap\nplt.tight_layout()\n",
        "\nd = np.random.random((10, 10))\n# Use matshow to plot d and make the figure size (8, 8)\nplt.figure(figsize=(8, 8))\nplt.imshow(d, cmap='gray')\n",
        "\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart. Show x axis tick labels on both top and bottom of the figure.\nplt.plot(x, y)\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart. Show x axis ticks on both top and bottom of the figure.\nplt.plot(x, y)\nplt.xlabel('x')\nplt.xticks(rotation=45)\nplt.ylabel('y')\nplt.yticks(rotation=45)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart. Show x axis tick labels but hide the x axis ticks\nplt.plot(x, y)\nplt.xticks(rotation=45)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y over x')\nplt.show()\n",
        "\ndf = sns.load_dataset(\"exercise\")\n# Make catplots of scatter plots by using \"time\" as x, \"pulse\" as y, \"kind\" as hue, and \"diet\" as col\n# Change the subplots titles to \"Group: Fat\" and \"Group: No Fat\"\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\nplt.title(\"Group: Fat\")\nplt.subplot(1, 2, 1)\nplt.title(\"Group: No Fat\")\nplt.show()\n",
        "\n# Set the xlabel to \"Exercise Time\"\nplt.xlabel(\"Exercise Time\")\n# Set the ylabel to \"Pulse\"\nplt.ylabel(\"Pulse\")\n",
        "\ndf = sns.load_dataset(\"exercise\")\n# Make catplots of scatter plots by using \"time\" as x, \"pulse\" as y, \"kind\" as hue, and \"diet\" as col\n# Do not show any ylabel on either subplot\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# plot y over x with label \"y\"\nplt.plot(x, y, label=\"y\")\n# make the legend fontsize 8\nplt.legend(fontsize=8)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x with figsize (5, 5) and dpi 300\nplt.plot(x, y, figsize=(5, 5), dpi=300)\n",
        "\n",
        "\n# To plot a, b, and c in the same figure, we can use the subplot function from matplotlib.\n# We can create a figure with three subplots, one for each of the functions.\nfig, axs = plt.subplots(1, 3, figsize=(10, 5))\n# Plot a in the first subplot\naxs[0].plot(t, a)\naxs[0].set_title('a = sin(t)')\naxs[0].set_xlabel('t')\naxs[0].set_ylabel('f(t)')\n# Plot b in the second subplot\naxs[1].plot(t, b)\naxs[1].set_title('b = cos(t)')\naxs[1].set_xlabel('t')\naxs[1].set_ylabel('f(t)')\n# Plot c in the third subplot\naxs[2].plot(t, c)\naxs[2].set_title('c = a + b')\naxs[2].set_xlabel('t')\naxs[2].set_ylabel('f(t)')\n",
        "\ndf = sns.load_dataset(\"penguins\")[[\"bill_length_mm\", \"species\", \"sex\"]]\n# Make a stripplot for the data in df. Use \"sex\" as x, \"bill_length_mm\" as y, and \"species\" for the color\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", data=df, hue=\"species\")\n# Remove the legend from the stripplot\nplt.legend(loc=\"upper left\")\n",
        "\ndf = pandas.DataFrame(\n    {\n        \"a\": np.arange(1, 31),\n        \"b\": [\"A\",] * 10 + [\"B\",] * 10 + [\"C\",] * 10,\n        \"c\": np.random.rand(30),\n    }\n)\n# Use seaborn FaceGrid for rows in \"b\" and plot seaborn pointplots of \"c\" over \"a\"\n# In each subplot, show xticks of intervals of 1 but show xtick labels with intervals of 2\nsns.facegrid(x=\"a\", hue=\"b\", data=df, sharey=True, ax=plt.gca())\nplt.xticks(rotation=45)\nplt.xticklabels(df[\"a\"].tolist(), interval=2)\nplt.show()\n",
        "\n# To change the view of the plot to have 100 azimuth and 50 elevation, we can use the view_init() method of the Axes3D object.\n# The first argument is the elevation angle in degrees, and the second argument is the azimuth angle in degrees.\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart and name axis with labels (\"x\" and \"y\")\n# Hide tick labels but keep axis labels\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n",
        "\nx = np.random.random((10, 10))\nfrom matplotlib import gridspec\nnrow = 2\nncol = 2\nfig = plt.figure(figsize=(ncol + 1, nrow + 1))\n# Make a 2x2 subplots with fig and plot x in each subplot as an image\n# Remove the space between each subplot and make the subplot adjacent to each other\n# Remove the axis ticks from each subplot\n# Create a grid of subplots\ngrid = gridspec.GridSpec(nrow, ncol, fig=fig, sharex=True, sharey=True)\n# Plot x in each subplot\nfor i in range(nrow):\n    for j in range(ncol):\n        plt.imshow(x[i, j], cmap='gray', origin='lower', extent=[j*fig.get_figwidth()/ncol, (j+1)*fig.get_figwidth()/ncol, i*fig.get_figheight()/nrow, (i+1)*fig.get_figheight()/nrow])\n        plt.axis('off')\n"
    ],
    "Tensorflow": [
        "\nimport tensorflow as tf\nx = tf.Variable(0)\nx.assign(1)\nresult = x\n",
        "\nimport tensorflow as tf\nx = tf.Variable(0)\nx.assign(114514)\nresult = x\n",
        "\nresult = tf.one_hot(labels, 10)\n",
        "\nresult = tf.one_hot(labels, 10)\n",
        "\nresult = tf.stack([tf.zeros((1, 10), dtype=tf.int32) for label in labels], axis=0)\nresult[labels] = 1\n",
        "\nimport tensorflow as tf\nexample_labels = [0, 6, 5, 4, 2]\ndef f(labels=example_labels):\n    result = tf.one_hot(labels, 10)\n    return result\n",
        "\nresult = tf.one_hot(labels, 10)\n",
        "\ndef my_map_func(i):\n  return [[i], [i+1], [i+2]]\n",
        "\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\nexample_input = [10, 20, 30]\ndef f(input=example_input):\n    result = []\n    for i in input:\n        result.extend(my_map_func(i))\n    return result\n",
        "\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\nresult = tf.pad(tf.ones(tf.shape(lengths)), [[0, 0], [0, 0]], padding='constant')\nprint(result)\n",
        "\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\nresult = tf.pad(tf.zeros(tf.shape(lengths)), [[0, 0], [0, 0]], padding='constant', constant_values=1)\nprint(result)\n",
        "\nmask = tf.zeros((len(lengths), 8), dtype=tf.int32)\nfor i in range(len(lengths)):\n    mask[i, :lengths[i]] = 1\nprint(mask)\n",
        "\nimport tensorflow as tf\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    result = tf.zeros((len(lengths), 8), dtype=tf.int32)\n    for i in range(len(lengths)):\n        result[i, :lengths[i]] = tf.ones(lengths[i], dtype=tf.int32)\n    return result\n",
        "\nmask = tf.zeros((len(lengths), 8), dtype=tf.float32)\nmask = tf.where(tf.range(8) < tf.reduce_sum(tf.ones_like(mask), axis=0), 1, 0)\nmask = tf.reshape(mask, (-1, 8))\n",
        "\nresult = tf.stack(tf.meshgrid(a, b), axis=1)\n",
        "\nimport tensorflow as tf\nexample_a = tf.constant([1,2,3])\nexample_b = tf.constant([4,5,6,7])\ndef f(a=example_a,b=example_b):\n    result = tf.stack(tf.meshgrid(a,b), axis=1)\n    return result\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 1, 512))\nresult = tf.reshape(a, (50, 100, 512))\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\nresult = a.reshape((50, 100, 1, 512))\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\nresult = a.reshape((1, 50, 100, 1, 512))\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100,size=(5, 3)))\nresult = tf.reduce_sum(A, axis=1)\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100,size=(5, 3)))\nresult = tf.reduce_sum(tf.math.log(tf.linalg.norm(A, axis=1)), axis=0)\nprint(result)\n",
        "\nimport tensorflow as tf\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\nresult = tf.reciprocal(A)\nprint(result)\n",
        "\nimport tensorflow as tf\na = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nb = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\nresult = tf.reduce_sum(tf.square(tf.sub(a, b)), axis=1)\nprint(result)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.sub(a, b)), axis=1)\n",
        "\nimport tensorflow as tf\nexample_a = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nexample_b = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\ndef f(A=example_a,B=example_b):\n    result = tf.reduce_sum(tf.square(tf.sub(A,B)),axis=1)\n    return result\n",
        "\nm = tf.gather(x, y, z)\n",
        "\nm = x[[row,col]]\n",
        "\nm = tf.gather(x, y, z)\n",
        "\nC = tf.matmul(A, tf.transpose(B))\n",
        "\nC = tf.matmul(A, tf.reshape(B, [-1, 1]))\n",
        "\nresult = tf.string.decode(x)\n",
        "\nimport tensorflow as tf\nexample_x=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\ndef f(x=example_x):\n    result = []\n    for i in range(len(x)):\n        result.append(x[i].decode())\n    return result\n",
        "\ny = tf.reduce_mean(x, axis=2) / tf.reduce_sum(tf.cast(tf.equal(x, 0), tf.float32), axis=2)\n",
        "\ny = tf.reduce_mean(x, axis=1)\n",
        "\ndef f(x=example_x):\n    result = tf.reduce_mean(x, axis=2)\n    result = tf.where(tf.is_not_zero(x), result, 0)\n    return result\n",
        "\nimport tensorflow as tf\ntry:\n    tf.compat.v1.Session = tf.Session\nexcept AttributeError:\n    tf.compat.v1.Session = tf.Session\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nwith tf.Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\nprint(result)\n",
        "\n# Convert the scores tensor so that each row simply contains the index of the highest value in each row.\n# Get the shape of the scores tensor\nscores_shape = scores.shape\n# Create a new tensor with the same shape as the scores tensor\nnew_scores = tf.zeros(scores_shape)\n# Iterate over each row in the scores tensor\nfor i in range(scores_shape[0]):\n    # Get the row from the scores tensor\n    row = scores[i]\n    # Find the index of the highest value in the row\n    max_index = tf.argmax(row)\n    # Set the value of the new tensor at the current row to the index of the highest value\n    new_scores[i] = max_index\n",
        "\n# Convert the scores tensor so that each row simply contains the index of the highest value in each column.\n# Create a new tensor with the same shape as the scores tensor.\n# Iterate over each row in the scores tensor.\n# For each row, find the index of the highest value in each column.\n# Store the indices in the new tensor.\n# Return the new tensor.\n",
        "\n# Convert the scores tensor so that each row simply contains the index of the highest value in each row.\n# Get the shape of the scores tensor\nscores_shape = scores.shape\n# Create a new tensor with the same shape as the scores tensor\nnew_scores = tf.zeros(scores_shape)\n# Iterate over each row in the scores tensor\nfor i in range(scores_shape[0]):\n    # Get the scores for the current row\n    row_scores = scores[i]\n    # Find the index of the highest value in the row\n    max_index = tf.argmax(row_scores)\n    # Set the value at the current index in the new tensor to the index of the highest value\n    new_scores[i, max_index] = i\n# Return the new tensor\nreturn new_scores\n",
        "\nresult = tf.argmin(a, axis=1)\n",
        "\nmodel.save(\"my_model\")\n",
        "\nimport tensorflow as tf\nseed_x = 10\n# Generate 10 random integers from {1, 2, 3, 4}\nresult = tf.random.uniform([10], 1, 4)\nprint(result)\n",
        "\nimport tensorflow as tf\nseed_x = 10\nresult = tf.random.uniform([114], 2, 5)\nprint(result)\n",
        "\nimport tensorflow as tf\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform([10], 1, 4)\n    return result\n",
        "\nimport tensorflow as tf\n### output the version of tensorflow into variable 'result'\nresult = tf.__version__\nprint(result)\n"
    ],
    "Scipy": [
        "\nimport numpy as np\nimport scipy\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\nresult = np.polyfit(x, y, 1)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\nresult = np.polyfit(x, y, 1)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.optimize\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\ndef func(x, p):\n    return p[0]*np.exp(p[1]*x) + p[2]\nresult = scipy.optimize.curve_fit(func, x, y, p0=p0)\nprint(result)\n",
        "\ntest_stat, p_value = stats.ttest_ind(x, y)\n",
        "\nfrom scipy import stats\nimport numpy as np\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\nalpha = 0.01\nresult = stats.ttest_ind(x, y)\nprint(result)\n",
        "\nimport scipy.optimize as optimize\nfrom math import *\ninitial_guess = [-1, 0, -3]\nresult = optimize.minimize(f, initial_guess)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.stats\nz_scores = np.array([-3, -2, 0, 2, 2.5])\np_values = scipy.stats.norm.ppf(z_scores)\nprint(p_values)\n",
        "\nimport scipy.stats\nimport numpy as np\nz_scores = [-3, -2, 0, 2, 2.5]\nmu = 3\nsigma = 4\np_values = [scipy.stats.norm.cdf(z, loc=mu, scale=sigma) for z in z_scores]\nprint(p_values)\n",
        "\nz_scores = np.arctan2(np.sqrt(-2 * np.log(p_values)), np.sqrt(1 - np.log(p_values)))\n",
        "\nimport numpy as np\nfrom scipy import stats\nstddev = 2.0785\nmu = 1.744\nx = 25\nresult = stats.lognorm.cdf(x, mu, stddev)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy import stats\nstddev = 2.0785\nmu = 1.744\nexpected_value = stats.lognorm.rvs(total, mu, stddev).mean()\nmedian = stats.lognorm.rvs(total, mu, stddev).median()\nprint(expected_value, median)\n",
        "\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\nresult = sa * sb\nprint(result)\n",
        "\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n    result = sA * sB\n    return result\n",
        "\nfrom scipy.interpolate import griddata\nresult = griddata(points, V, request, method='cubic')\n",
        "\n# Create a grid of points\ngrid = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n# Create a 3D array of points\npoints3d = np.column_stack((grid[:, :, 0], grid[:, :, 1], grid[:, :, 2], V))\n# Create a 3D interpolation function\ninterp = scipy.interpolate.LinearNDInterpolator(points3d)\n# Evaluate the interpolation function at the request points\nresult = interp(request)\nprint(result)\n",
        "\n# Translate the image to the center of the image array\ncenter = data_orig.shape[1] // 2, data_orig.shape[0] // 2\ndata_rot = np.roll(data_rot, center, axis=(0, 1))\n# Rotate the image by the specified angle\ndata_rot = rotate(data_rot, angle)\n# Translate the image back to its original position\ndata_rot = np.roll(data_rot, -center, axis=(0, 1))\n# Find the rotated frame coordinates (x',y')\nxrot, yrot = np.where(data_orig == 255)\nxrot = xrot - center[0]\nyrot = yrot - center[1]\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\nresult = np.diag(M)\nprint(result)\n",
        "\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\nresult = stats.kstest(times, \"uniform\")\nprint(result)\n",
        "\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nexample_rate = 1.0\nexample_T = 100.0\nexample_times = poisson_simul(example_rate, example_T)\ndef f(times = example_times, rate = example_rate, T = example_T):\n    result = stats.kstest(times, \"uniform\")\n    return result\n",
        "\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\nresult = stats.kstest(times, \"uniform\")[1]\nprint(result)\n",
        "\nFeature = sparse.vstack((c1, c2))\n",
        "\nFeature = sparse.hstack((c1, c2))\n",
        "\nFeature = c1.hstack(c2)\n",
        "\nimport numpy as np\nimport scipy.spatial\nimport scipy.optimize\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n# Initialize the assignment matrix with zeros\nresult = np.zeros(N, dtype=int)\n# Define the objective function to minimize\ndef objective(x):\n    distances = []\n    for i in range(N):\n        for j in range(N):\n            if x[i] != x[j]:\n                distance = np.linalg.norm(points1[i] - points1[j])\n                distances.append(distance)\n    return np.sum(distances)\n# Use the linear programming solver to find the optimal assignment\nresult = scipy.optimize.linprog(objective, bounds=(0,1), ncol=N)[:,0]\nprint(result)\n",
        "\n# Define the objective function to minimize\ndef objective(x):\n    return np.sum(np.linalg.norm(points1[:, np.newaxis] - points2[np.newaxis, :], axis=1))\n# Define the constraints to ensure each point is only used once\ndef constraints(x):\n    return np.sum(x, axis=1) == 1\n# Use the linear programming solver to find the optimal solution\nresult = scipy.optimize.linprog(objective, constraints=constraints, bounds=(0, 1))[0]\n",
        "\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\nb.setdiag(0)\nprint(b)\n",
        "\ndef count_regions(img, threshold):\n    n_regions = 0\n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            if img[i,j] > threshold:\n                n_regions += 1\n                while True:\n                    if i > 0 and img[i-1,j] <= threshold:\n                        break\n                    if j > 0 and img[i,j-1] <= threshold:\n                        break\n                    if i < img.shape[0]-1 and img[i+1,j] <= threshold:\n                        break\n                    if j < img.shape[1]-1 and img[i,j+1] <= threshold:\n                        break\n                if i > 0 and j > 0 and img[i-1,j-1] <= threshold:\n                    break\n                if i > 0 and j < img.shape[1]-1 and img[i-1,j+1] <= threshold:\n                    break\n                if j > 0 and i < img.shape[0]-1 and img[i+1,j-1] <= threshold:\n                    break\n                if j > 0 and i < img.shape[0]-1 and img[i+1,j+1] <= threshold:\n                    break\n                if i < img.shape[0]-1 and j < img.shape[1]-1 and img[i+1,j+1] <= threshold:\n                    break\n    return n_regions\nresult = count_regions(img, threshold)\n",
        "\ndef count_regions(img, threshold):\n    n_regions = 0\n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            if img[i,j] < threshold:\n                n_regions += 1\n    return n_regions\nresult = count_regions(img, threshold)\n",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nexample_img /= img.max()\ndef f(img = example_img):\n    threshold = 0.75\n    # Initialize a 2D array to store the number of regions\n    num_regions = np.zeros((512, 512))\n    # Loop through each pixel in the image\n    for y in range(512):\n        for x in range(512):\n            # Check if the pixel value exceeds the threshold\n            if img[y, x] > threshold:\n                # If it does, increment the count of the current region\n                num_regions[y, x] += 1\n    # Return the number of regions\n    return num_regions\n",
        "\n# Find the regions of cells which value exceeds a given threshold, say 0.75;\n# Note: If two elements touch horizontally, vertically or diagnoally, they belong to one region.\n# Determine the distance between the center of mass of such regions and the top left corner, which has coordinates (0,0).\n",
        "\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy import sparse\nM= sparse.random(10, 10, density=0.1, format='lil')\nM.make_symmetric()\nprint(M)\n",
        "\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nexample_sA = sparse.random(10, 10, density=0.1, format='lil')\ndef f(sA = example_sA):\n    sA = sA.make_symmetric()\n    return sA\n",
        "\nimport numpy as np\nimport scipy.ndimage\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = 1\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = 1\n# [Missing Code]\n# Use the scipy.ndimage.binary.bwareaopen function to remove completely isolated single cells\n# Set \"min_area\" to 1 to remove all single cells\nsquare = scipy.ndimage.binary.bwareaopen(square, min_area=1)\n",
        "\nimport numpy as np\nimport scipy.ndimage\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = np.random.randint(1, 255, size = (12, 12))\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = np.random.randint(1, 255, size = (20,))\n# [Missing Code]\n# Use the scipy.ndimage.morphology.binary_op function to perform binary erosion and dilation on the square array.\n# Set the structuring element to be a disk of radius 1.\n# The binary_op function takes two arguments: the array to be operated on and the structuring element.\n# The first argument is the array to be operated on, and the second argument is the structuring element.\n# The function returns the eroded or dilated array.\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\nmean = np.mean(col)\nstandard_deviation = np.std(col)\n",
        "\n# max_val = np.max(col)\n# min_val = np.min(col)\n",
        "\nmedian = np.median(col.toarray())\nmode = np.mode(col.toarray())\n",
        "\n    return sum([a_i * np.cos(i * np.pi / tau * x) for i in range(1, n+1)])\n",
        "\nresult = np.zeros((len(example_array), len(example_array)))\nfor i in range(len(example_array)):\n    for j in range(len(example_array)):\n        if i == j:\n            result[i, j] = 0\n        else:\n            result[i, j] = scipy.spatial.distance.euclidean(example_array[i], example_array[j])\n",
        "\nresult = np.zeros((len(example_array), len(example_array)))\nfor i in range(len(example_array)):\n    for j in range(len(example_array)):\n        if i == j:\n            result[i, j] = 0\n        else:\n            result[i, j] = scipy.spatial.distance.euclidean(example_array[i], example_array[j])\n",
        "\ndef f(example_array = example_arr):\n    distances = scipy.spatial.distance.cdist(example_array, metric='euclidean')\n    result = np.zeros((len(example_array), len(example_array)))\n    for i in range(len(example_array)):\n        for j in range(i+1, len(example_array)):\n            result[i,j] = distances[i,j]\n            result[j,i] = distances[i,j]\n    return result\n",
        "\ntck = interpolate.splrep(x, y, k = 2, s = 4)\ny_int = interpolate.splev(x_val, tck, der = 0)\n",
        "\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x1, x2, x3, x4)\n",
        "\nresult, p = ss.anderson_ksamp(x1, x2, alpha=0.05)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\ndef tau1(x):\n    y = np.array(df['A'])  #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\ndf['AC'] = pd.rolling_apply(df['A'], 3, lambda x: tau1(x))\ndf['BC'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\nprint(df)\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = sa.empty\nprint(result)\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'lil')\nresult = sa.issparse()\nprint(result)\n",
        "\nresult = np.zeros((len(a), len(a[0]), len(a[0][0])))\nfor i in range(len(a)):\n    for j in range(len(a[i])):\n        for k in range(len(a[i][j])):\n            result[i,j,k] = a[i][j][k]\nprint(result)\n",
        "\np_value = stats.ranksums(pre_course_scores, during_course_scores)[1]\n",
        "\np_value = stats.ranksums(pre_course_scores, during_course_scores)[1]\n",
        "\nimport numpy as np\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\nkurtosis_result = np.mean(np.power(a - np.mean(a), 4))\nprint(kurtosis_result)\n",
        "\nimport numpy as np\nimport scipy.stats\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\nkurtosis_result = scipy.stats.kurtosis(a)\nprint(kurtosis_result)\n",
        "\nresult = np.interpolate.interp2d(np.column_stack((s, t)), np.column_stack((x, y, z)), s1, s2, t1, t2)\n",
        "\nimport numpy as np\nimport scipy.interpolate\nexampls_s = np.linspace(-1, 1, 50)\nexample_t = np.linspace(-2, 0, 50)\ndef f(s = example_s, t = example_t):\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n    result = np.interp(s, x, z)\n    result = np.interp(t, y, result)\n    return result\n",
        "\nimport scipy.spatial\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Get the indices of the extra points in the voronoi regions\nresult = np.array([vor.region[i] for i in range(len(extraPoints))])\nprint(result)\n",
        "\nresult = np.zeros(len(vor.regions), dtype=int)\nfor i, point in enumerate(extraPoints):\n    region = vor.query_point(point)\n    result[region] += 1\n",
        "\nimport numpy as np\nimport scipy.sparse as sparse\nnp.random.seed(10)\nmax_vector_size = 1000\nvectors = [np.random.randint(100,size=900),np.random.randint(100,size=max_vector_size),np.random.randint(100,size=950)]\n# Create a sparse matrix using the vectors\nresult = sparse.csc_matrix(vectors)\n# Pad the vectors with zeros to the maximum size\nfor i in range(len(vectors)):\n    vectors[i] = np.pad(vectors[i], max_vector_size - len(vectors[i]), 'constant', constant_value=0)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.ndimage\na= np.zeros((5, 5))\na[1:4, 1:4] = np.arange(3*3).reshape((3, 3))\nb = scipy.ndimage.filters.median_filter(a, 3, origin=1)\nprint(b)\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = 2\ncolumn = 3\nresult = M.getrow(row)[column]\nprint(result)\n",
        "\nresult = []\nfor row_idx, col_idx in zip(row, column):\n    row_vec = M.getrow(row_idx)\n    result.append(row_vec[col_idx])\n",
        "\nnew_array = np.zeros((1000, 10, 10))\nf = interp1d(x, array)\nnew_array = f(x_new)\n",
        "\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\nx = 2.5\nu = 1\no2 = 3\nP_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\nP = P_inner + P_outer/2\nprint(P)\n",
        "\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\ndef f(x = 2.5, u = 1, o2 = 3):\n    # Begin of Missing Code\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    # End of Missing Code\n    prob = P_inner + P_outer/2\n    return(prob)\n",
        "\nimport numpy as np\nimport scipy.fft as sf\nN = 8\n# Compute the DCT matrix\nresult = sf.dctn(np.eye(N))\n# Normalize the matrix\nresult = result / np.linalg.norm(result, axis=1)[:, np.newaxis]\n# Compute the inverse DCT matrix\nresult = sf.idctn(result)\nprint(result)\n",
        "\nfrom scipy import sparse\nimport numpy as np\nmatrix = np.array([[3.5,   13. ,   28.5,   50. ,   77.5],\n                   [-5. ,  -23. ,  -53. ,  -95. , -149. ],\n                   [2.5,   11. ,   25.5,   46. ,   72.5]])\nresult = sparse.diag(matrix, offset=[-1,0,1], k=5)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.stats\nN = 3\np = 0.5\nresult = np.zeros((N+1, N+1))\nfor i in range(N+1):\n   for j in range(i+1):\n      result[i,j] = scipy.stats.choose(i, j) * p**j * (1-p)**(i-j)\nprint(result)\n",
        "\ndf['sample1_zscore'] = stats.zscore(df['sample1'])\ndf['sample2_zscore'] = stats.zscore(df['sample2'])\ndf['sample3_zscore'] = stats.zscore(df['sample3'])\nresult = df[['sample1_zscore', 'sample2_zscore', 'sample3_zscore']]\n",
        "\ndf['sample1_zscore'] = stats.zscore(df['sample1'])\ndf['sample2_zscore'] = stats.zscore(df['sample2'])\ndf['sample3_zscore'] = stats.zscore(df['sample3'])\n",
        "\ndf['zscore'] = df['sample1'].apply(lambda x: stats.zscore(x))\ndf['zscore'] = df['zscore'].apply(lambda x: stats.zscore(x))\ndf['zscore'] = df['zscore'].apply(lambda x: stats.zscore(x))\n",
        "\ndf['zscore'] = df['sample1'].apply(lambda x: stats.zscore(x))\ndf['zscore'] = df['sample2'].apply(lambda x: stats.zscore(x))\ndf['zscore'] = df['sample3'].apply(lambda x: stats.zscore(x))\ndf['data'] = df['sample1'].apply(lambda x: round(x, 3))\ndf['data'] = df['sample2'].apply(lambda x: round(x, 3))\ndf['data'] = df['sample3'].apply(lambda x: round(x, 3))\nresult = df[['data', 'zscore']]\n",
        "\nimport scipy\nimport scipy.optimize\nimport numpy as np\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\nstarting_point = [1.8, 1.7]\ndirection = [-1, -1]\nresult = scipy.optimize.line_search(test_func,test_grad,starting_point,direction)\nprint(result)\n",
        "\nmid = np.zeros(shape, dtype=np.float32)\nmid[:] = shape[0] / 2\n",
        "\nmid = np.array([[0, 0], [0, 0]])\nresult = get_distance_2(y, x)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # Create a 2D array of shape (rows, cols, 2)\n    mid = np.zeros(shape, dtype=np.float32)\n    # Compute distances from center point to every point in the image\n    result = distance.cdist(np.dstack((np.zeros(shape, dtype=np.float32), np.zeros(shape, dtype=np.float32))), mid)\n    return result\n",
        "\nimport numpy as np\nimport scipy.ndimage\nx = np.arange(9).reshape(3, 3)\nshape = (6, 8)\nresult = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n",
        "\n# Define the objective function\ndef objective(x):\n    return np.dot(a, x**2)\n",
        "\nimport scipy.optimize\nimport numpy as np\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\nx_lower_bounds = x_true / 2\nout = minimize(residual, x0, args=(a, y), method='L-BFGS-B', bounds=(x_lower_bounds, None))\nprint(out)\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\ndef dN1_dt_sin(t, N1):\n    return -100*N1 + np.sin(t)\nsol = solve_ivp(fun=dN1_dt_sin, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 1\ntime_span = [0, 10]\ndef dN1_dt_sin(t, N1):\n    return -100 * N1 + t - np.sin(t)\nsol = solve_ivp(fun=dN1_dt_sin, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\ndef dN1_dt_sinusoid(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = solve_ivp(fun=dN1_dt_sinusoid, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = sparse.hstack((sa, sb))\nprint(result)\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = sparse.hstack((sa, sb))\nprint(result)\n",
        "\nimport scipy.integrate\nc = 5\nlow = 0\nhigh = 1\nresult = []\nfor c in range(1, 11):\n    result.append(scipy.integrate.quad(lambda x: 2*x*c, low, high)[0])\nprint(result)\n",
        "\nimport scipy.integrate\ndef f(c=5, low=0, high=1):\n    result,error = integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n",
        "\nimport numpy as np\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'dok', random_state = 42)\nx = 99\nV = V + x * V.nonzero()\nprint(V)\n",
        "\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\nV = V + x * V.nonzero()\nprint(V)\n",
        "\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\ny = 99\n# Create a new sparse matrix with the same format as V\nW = sparse.eye(V.shape[0], format = 'coo')\n# Iterate over the non-zero values in V and add x to them\nfor i, j, v in V.nonzero():\n    W[i, j] += x\n# Add y to W\nW += y\n# Print the resulting matrix\nprint(W)\n",
        "\n# [Missing Code]\n",
        "\n# Update the original column of the matrix\nsa.data[Col] = (1/Len)*sa.data[Col]\n",
        "\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\na = np.where(a > 0, 1, 0)\nprint(a)\n",
        "\nimport scipy\nimport numpy as np\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\na = np.where(a == 0, 0, 1)\nprint(a)\n",
        "\nresult = []\nfor i in range(5):\n    cluster_idx = np.where(labels == i)[0]\n    closest_idx = np.argmin([np.linalg.norm(data[j] - centroids[i]) for j in cluster_idx])\n    result.append(closest_idx)\n",
        "\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n# Compute the distance matrix\ndist_matrix = scipy.spatial.distance.euclidean(data, centroids)\n# Cluster the data using hierarchical clustering\nZ = linkage(dist_matrix, method='ward')\n# Cut the tree into clusters\nlabels = cut_tree(Z, k=5)\n# Find the closest point to each cluster\nresult = []\nfor i in range(5):\n    cluster_indices = np.where(labels == i)[0]\n    closest_point = data[cluster_indices[0]]\n    result.append(closest_point)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nk = 3\n# Compute the distance matrix\ndist_matrix = scipy.spatial.distance.euclidean(data, centroids)\n# Cluster the data using hierarchical clustering\nZ = linkage(dist_matrix, method='ward')\n# Cut the tree into clusters\nlabels = cut_tree(Z, k=k)\n# Initialize an empty list to store the indices of the k-closest elements to each cluster\nresult = []\n# Iterate over each cluster and find the k-closest element\nfor i in range(k):\n    cluster_indices = np.where(labels == i)[0]\n    cluster_data = data[cluster_indices]\n    distances = scipy.spatial.distance.euclidean(cluster_data, centroids[i])\n    closest_indices = np.argsort(distances)[::-1][:k]\n    result.append(closest_indices)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\nresult = fsolve(lambda x, a, b: eqn(x, a, b), x0=0.5, args=(a,b), x=xdata, y=bdata)\nprint(result)\n",
        "\nresult = []\nfor i in range(len(xdata)):\n    x0 = xdata[i]\n    args = (adata[i],)\n    roots = fsolve(eqn, x0=x0, args=args)\n    result.append(roots)\n",
        "\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n# [Missing Code]\n# Compute the sample density function using the fitted function\nsample_density = bekkers(sample_data, estimated_a, estimated_m, estimated_d)\n# [Missing Code]\n# Compute the sample distribution function using the fitted function\nsample_dist = stats.rvs_continuous(low=range_start, high=range_end, size=len(sample_data), loc=estimated_m, scale=estimated_d)\n# [Missing Code]\n# Compute the sample distribution function using the fitted function\nsample_dist = stats.rvs_continuous(low=range_start, high=range_end, size=len(sample_data), loc=estimated_m, scale=estimated_d)\n",
        "\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n# [Missing Code]\n# Compute the KS test statistic and p-value\n",
        "\n# Create a rolling window of size 5\nwindow = df.rolling(window_size=5)\n",
        "\nimport scipy.interpolate\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\neval = [(2.7, 2.3)]\nresult = griddata(x, y, eval, method='cubic')\n",
        "\nimport scipy.optimize as sciopt\nimport numpy as np\nimport pandas as pd\na=pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\n# Define the likelihood function\ndef likelihood(params, data):\n    weights = np.zeros(len(params))\n    for i in range(len(params)):\n        weights[i] = np.sum(data == i) / len(data)\n    return np.sum(data * np.log(weights))\n# Define the objective function to minimize\ndef objective(params):\n    return -sciopt.minimize(likelihood, params, method='SLSQP')['fun']\n# Find the best parameters using Maximum Likelihood Estimation\nparams = sciopt.minimize(objective, np.ones(len(a['A1'].unique())), method='SLSQP')['x']\n# Print the best parameters\nprint(params)\n",
        "\npopt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nprint(popt)\n",
        "\nimport numpy as np\nfrom scipy import signal\narr = np.array([-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,])\nn = 2\n# Find the indices of relative extrema\nresult = []\nfor i in range(len(arr)-n):\n    if all(arr[i+j] >= arr[i] for j in range(n)) or all(arr[i+j] <= arr[i] for j in range(n)):\n        result.append(i)\nprint(result)\n",
        "\ndef find_relative_extrema(arr, n):\n    result = []\n    for i in range(len(arr)):\n        for j in range(len(arr[i])):\n            if j > 0 and j < len(arr[i]) - n and arr[i][j] <= arr[i][j-1] and arr[i][j] <= arr[i][j+1]:\n                result.append([i, j])\n    return result\n",
        "\ndf = df[(np.abs(stats.zscore(df[['NUM1', 'NUM2', 'NUM3']])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\nprint(data1)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col1', 'Col2', 'Col3'])\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col2'])\ndf_out = df_out.drop(['Col1'], axis=1)\ndf_out = df_out.rename(columns={'Col2': 'Apple', 'Col3': 'Orange', 'Col4': 'Banana', 'Col5': 'Grape'})\n",
        "\ndf_out = pd.get_dummies(df['Col4'])\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col3'])\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col3'])\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.svm as suppmach\nX, y, x_test = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(x_test) == np.ndarray\n# Fit model:\nsvmmodel=suppmach.LinearSVC()\n# Probability estimates using logistic function\nproba = 1 / (1 + np.exp(-svmmodel.decision_function(x_test)))\nprint(proba)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import svm\nX, y, x_predict = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(x_predict) == np.ndarray\nmodel = svm.LinearSVC()\nproba = CalibratedClassifierCV(model, cv=5).predict_proba(x_predict)\n",
        "\ndf = pd.concat([df_origin, transform_output], axis=1)\n",
        "\ndf = pd.concat([df_origin, transform_output], axis=1)\n",
        "\ndf_merged = pd.concat([df_origin, transform_output], axis=1)\n",
        "\nclf.steps.remove('poly')\n",
        "\nclf.steps.remove('reduce_poly')\nclf.steps.remove('dim_svm')\nclf.steps.remove('sVm_233')\n",
        "\nclf.steps = clf.steps[:-1]\n",
        "\nclf.steps.append('new_step')\nclf.steps.remove('reduce_dim')\n",
        "\nclf.steps.append('new_step')\nclf.steps.remove('dim_svm')\n",
        "\nclf.steps.insert(1, 't1919810', PCA())\n",
        "",
        "\ngridsearch.fit(trainX, trainY, **fit_params)\n",
        "\nproba = []\nfor train_index, test_index in cv.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5).split(X, y)\nlogreg = LogisticRegression()\nproba = []\nfor train_index, test_index in cv.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    y_pred = logreg.predict(X_test)\n    proba.append(logreg.predict_proba(X_test))\nprint(proba)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\n# [Missing Code]\n# Inverse StandardScaler\ninversed = scaler.inverse_transform(scaled)\n",
        "\n# [Missing Code]\n",
        "\nmodel_name = model.get_class().__name__\n",
        "\nmodel_name = model.get_class_name()\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\ndata, target = load_data()\npipe = Pipeline(steps=[\n    ('select', SelectKBest(k=2)),\n    ('clf', LogisticRegression())]\n)\nselect_out = pipe.fit_transform(data, target)\nprint(select_out)\n",
        "\ngrid_search = GridSearchCV(estimator=bc, param_grid=param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\n",
        "\n# Create a DataFrame from X and y\ndf = pd.DataFrame({'X': X, 'y': y})\n# Split the DataFrame into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df['X'], df['y'], test_size=0.2, random_state=42)\n# Create a RandomForestRegressor object\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\n# Fit the RandomForestRegressor object to the training data\nregressor.fit(X_train, y_train)\n",
        "\n# Create a DataFrame with X and y data\ndf = pd.DataFrame({'X': X, 'y': y})\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef preprocess(s):\n    return s.upper()\ntfidf = TfidfVectorizer(preprocessor=preprocess)\nprint(tfidf.preprocessor)\n",
        "\ntfidf.preprocessor = prePro\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns)\nprint(df_out)\n",
        "\ncoef = grid.best_params_\nprint(coef)\n",
        "\ncoef = grid.best_params_\nprint(coef)\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n# Get selected columns names from SelectFromModel method\ncolumn_names = np.array(clf.feature_names_)\nprint(column_names)\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n# Begin of Missing Code\nselected_features = clf.feature_names_out_\ncolumn_names = pd.Series(selected_features)\n# End of Missing Code\nprint(column_names)\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n# Get selected column names\ncolumn_names = clf.feature_names_out_\nX_new = SelectFromModel(clf, prefit=True).transform(X)\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n# Get selected columns names from SelectFromModel method\ncolumn_names = pd.Series(clf.feature_names_, index=clf.feature_importances_)\n# Select important features as a list\nselected_features = column_names[column_names > 0].tolist()\n",
        "\nclosest_50_samples = []\nfor i in range(p):\n    closest_samples = km.cluster_centers_[i]\n    closest_samples = np.argsort(np.linalg.norm(X - closest_samples, axis=1))[:50]\n    closest_50_samples.append(closest_samples)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nclosest_50_samples = km.cluster_centers_[p-1]\nprint(closest_50_samples)\n",
        "\n# Get the cluster centers\ncenters = km.cluster_centers_\n# Find the indices of the 100 closest samples to the p^th center\nclosest_indices = np.argsort(np.linalg.norm(X - centers[p], axis=1))[:100]\n# Get the corresponding samples\nclosest_samples = X[closest_indices]\n",
        "\n# Get the cluster labels for each sample\nlabels = km.labels_\n# Get the indices of the samples belonging to the p^th cluster\npth_cluster_indices = np.where(labels == p)[0]\n# Get the corresponding samples from the original data\nclosest_50_samples = X[pth_cluster_indices[:50]]\n",
        "\nX_train = pd.get_dummies(X_train, columns=['0'])\n",
        "\nX_train = pd.get_dummies(X_train, columns=['0'])\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n# fit, then predict X\nsvm = SVR(kernel='rbf')\nsvm.fit(X, y)\ny_pred = svm.predict(X)\n# calculate mean squared error\nmse = mean_squared_error(y, y_pred)\nprint(mse)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_approximation import RBFSampler\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\n# fit, then predict X\nsvm = SVR(kernel='rbf', C=1.0, gamma=0.1)\nsampler = RBFSampler(n_components=100, random_state=42)\nX_sampled = sampler.fit_transform(X)\ny_pred = svm.predict(X_sampled)\nprint(y_pred)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nclf = SVR(kernel='rbf')\nclf.fit(X_poly, y)\npredict = clf.predict(X_poly)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\n# fit, then predict X\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nclf = SVR(kernel='rbf')\nclf.fit(X_poly, y)\ny_pred = clf.predict(X_poly)\nprint(y_pred)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ntfidf = TfidfVectorizer()\ntfidf.fit_transform(documents)\ncosine_similarities_of_queries = np.dot(tfidf.transform(queries), tfidf.transform(documents).T)\nprint(cosine_similarities_of_queries)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ntfidf = TfidfVectorizer()\ntfidf.fit_transform(documents)\ncosine_similarities_of_queries = np.dot(tfidf.transform(queries), tfidf.transform(documents).T)\nprint(cosine_similarities_of_queries)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    similarities = np.dot(tfidf.transform(queries), tfidf.transform(documents).T)\n    return similarities\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\n# Convert features to a 2D array\nnew_features = pd.DataFrame(features, columns=['f' + str(i) for i in range(1, 7)])\nnew_features = new_features.fillna(0)\nnew_features = new_features.astype(np.float32)\nprint(new_features)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nf = load_data()\n# Convert features to a 2D array\nnew_f = pd.DataFrame(f, columns=['t1', 't2', 't3', 't4', 't5', 't6', 't7'])\nnew_f = new_f.fillna(0)\nnew_f = new_f.astype(int)\nprint(new_f)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\n# Convert features to a 2D array\nnew_features = pd.DataFrame(features, columns=['f' + str(i) for i in range(1, 7)])\n# One hot encode features\nnew_features = pd.get_dummies(new_features, columns=['f' + str(i) for i in range(1, 7)])\nprint(new_features)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\ndef solve(features):\n    # Convert features to a 2D array\n    new_features = np.array(features).reshape(-1, 1)\n    # One hot encode the features\n    new_features = pd.get_dummies(new_features)\n    return new_features\nnew_features = solve(features)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\n# Convert features to a 2D array\nnew_features = pd.DataFrame(features, columns=['t1', 't2', 't3', 't4', 't5', 't6', 't7'])\nnew_features = new_features.fillna(0)\nnew_features = new_features.astype(np.float32)\nprint(new_features)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.cluster\ndata_matrix = load_data()\n# Convert the data matrix to a distance matrix\ndist_matrix = pd.DataFrame(data_matrix, index=['prof1', 'prof2', 'prof3'], columns=['prof1', 'prof2', 'prof3'])\ndist_matrix = dist_matrix.apply(lambda x: x.apply(lambda y: np.sqrt(np.sum((x - y) ** 2))))\n# Perform hierarchical clustering using AgglomerativeClustering\nclusterer = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\ncluster_labels = clusterer.fit_predict(dist_matrix)\nprint(cluster_labels)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.cluster\ndata_matrix = load_data()\nfrom sklearn.cluster import AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2)\nclustering.fit(data_matrix)\ncluster_labels = clustering.labels_\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.cluster\nsimM = load_data()\n# Convert the distance matrix to a distance matrix\ndistM = np.sqrt(np.diag(np.square(simM)))\n# Perform hierarchical clustering using AgglomerativeClustering\nclusterer = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\nclusterer.fit(distM)\n# Get the cluster labels\ncluster_labels = clusterer.labels_\nprint(cluster_labels)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\ndata_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\nZ = sch.linkage(data_matrix, method='ward')\n# [Missing Code]\ncluster_labels = sch.cutree(Z, k=2)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\ndata_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\nZ = sch.linkage(data_matrix, method='ward')\n# [Missing Code]\ncluster_labels = sch.cutree(Z, k=2)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nsimM = load_data()\nZ = sch.linkage(simM, method='ward')\nlabels = np.array_split(sch.cutree(Z, k=2), 2)[0]\nprint(labels)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Standardize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n# Center the data\ncentered_data = np.mean(data, axis=0)\n# Scale the data\nscaled_data = np.std(data, axis=0)\n# Combine the three transformations\ntransformed_data = np.dot(centered_data.reshape(-1, 1), scaled_data.reshape(1, -1))\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Scale the data\nscaled_data = sklearn.preprocessing.StandardScaler().fit_transform(data)\n# Center the data\ncentered_data = np.mean(scaled_data, axis=0)\n# Combine scaling and centering\ncentered_scaled_data = sklearn.preprocessing.StandardScaler().fit_transform(data)\nprint(centered_scaled_data)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Apply Box-Cox transformation to the data\nbox_cox_data = sklearn.preprocessing.BoxCox(alpha=0.5).fit_transform(data)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Box-Cox transformation\nbox_cox_data = sklearn.preprocessing.BoxCoxTransformer().fit_transform(data)\nprint(box_cox_data)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Yeo-Johnson Transformation\ndef yeo_johnson_transform(data):\n    mu = np.mean(data, axis=0)\n    sigma = np.std(data, axis=0)\n    return np.power(data - mu, 1/3) * sigma\nyeo_johnson_data = yeo_johnson_transform(data)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Yeo-Johnson Transformation\nyeo_johnson_data = sklearn.preprocessing.yeo_johnson(data)\nprint(yeo_johnson_data)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ntext = load_data()\nvectorizer = CountVectorizer(stop_words='english', max_features=5000)\ntransformed_text = vectorizer.fit_transform(text)\nprint(transformed_text)\n",
        "\nimport numpy as np\nimport pandas as pd\ndataset = load_data()\n# Split the dataset into training and testing sets\ntrain_size = int(len(dataset) * 0.8)\ntrain_data = dataset.iloc[:train_size]\ntest_data = dataset.iloc[train_size:]\n# Split each set into x and y\nx_train = train_data.iloc[:, :-1]\ny_train = train_data.iloc[:, -1]\nx_test = test_data.iloc[:, :-1]\ny_test = test_data.iloc[:, -1]\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\ndata = load_data()\n# Split the dataframe into training and testing sets\nX = data.iloc[:, :-1] # all columns except the last one\ny = data.iloc[:, -1] # the last column\n# Split the training set into 80% and 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Split the testing set into 80% and 20%\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=42)\nprint(X_train)\nprint(y_train)\nprint(X_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\ndataset = load_data()\n# Split the dataset into training and testing sets\ntrain_size = int(len(dataset) * 0.8)\ntrain_data = dataset.iloc[:train_size]\ntest_data = dataset.iloc[train_size:]\n# Split each set into x and y\nx_train = train_data.iloc[:, :-1]\ny_train = train_data.iloc[:, -1]\nx_test = test_data.iloc[:, :-1]\ny_test = test_data.iloc[:, -1]\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\ndataset = load_data()\ndef solve(data):\n    # Split the dataset into training and testing sets\n    train_size = int(len(data) * 0.8)\n    train_data = data.iloc[:train_size]\n    test_data = data.iloc[train_size:]\n    \n    # Split each set into x and y\n    x_train = train_data.iloc[:, :-1]\n    y_train = train_data.iloc[:, -1]\n    x_test = test_data.iloc[:, :-1]\n    y_test = test_data.iloc[:, -1]\n    \n    return x_train, y_train, x_test, y_test\nx_train, y_train, x_test, y_test = solve(dataset)\n",
        "\nfrom sklearn.cluster import KMeans\ndf = load_data()\nX = df['mse'].values\n# [Missing Code]\n# [Missing Code]\n",
        "\n# Create a 2D array of the mse and range values\nX = np.array([df['mse'].values, list(range(0, len(df['mse'].values)))]).T\n# Fit the KMeans model to the data\nkmeans = KMeans(n_clusters=2).fit(X)\n# Get the cluster labels for each data point\nlabels = kmeans.predict(X)\n# Get the cluster centroids\ncentroids = kmeans.cluster_centers_\n",
        "\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[vectorizer.transform(X).sum(axis=0) > 0]\n",
        "\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.get_support()]\n",
        "\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[X.sum(axis=0) > 0]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n",
        "\n# Set the order of the vocabulary\nvectorizer.fit(corpus)\nvectorizer.vocabulary_ = vectorizer.get_feature_names()\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript', '.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\n# [Missing Code]\n# vectorizer.fit(corpus)\n# X = vectorizer.transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = vectorizer.fit_transform(corpus)\nprint(feature_names)\nprint(X)\n",
        "\n# Get the feature names in the order they were set in the vocabulary\nfeature_names = vectorizer.get_feature_names()\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\n# Initialize an empty list to store the slope coefficients\nslopes = []\n# Iterate over each column in the dataframe\nfor col in df1.columns:\n    # Remove NaN values for the current column\n    df2 = df1[~np.isnan(df1[col])]\n    \n    # Create a new dataframe with only the 'Time' and current column\n    df3 = df2[['Time', col]]\n    \n    # Convert the dataframe to a numpy matrix\n    npMatrix = np.matrix(df3)\n    \n    # Split the matrix into input (X) and output (Y) variables\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    \n    # Fit a linear regression model to the data\n    slope = LinearRegression().fit(X,Y)\n    \n    # Append the slope coefficient to the list\n    slopes.append(slope.coef_[0])\n# Convert the list to a numpy array and print it\nprint(np.array(slopes))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\nprint(slopes)\n",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\nElasticNet = sklearn.linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\nprint ('-'*50)\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n",
        "\nclose_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict([b])\n",
        "\n# [Missing Code]\n",
        "\nnew_X = np.array(X)\nnew_X = new_X[:, :-1]\nclf.fit(new_X, ['2', '3'])\n",
        "\nnew_X = np.array(X)\nnew_X = new_X.astype('category')\nclf.fit(new_X, ['4', '5'])\n",
        "\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].values\ny = dataframe.iloc[:, -1].values\n",
        "\nX = dataframe.iloc[:, :-1].values\ny = dataframe.iloc[:, -1].values\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\n# Split the dataframe into train and test sets\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n# Sort the train and test dataframes by date\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n# Get the indices of the first 20% and last 80% of the dataframe\ntrain_indices = train_dataframe.index[:int(len(train_dataframe)*train_size)]\ntest_indices = train_dataframe.index[int(len(train_dataframe)*train_size):]\n# Select the rows corresponding to the train and test indices\ntrain_dataframe = train_dataframe.iloc[train_indices]\ntest_dataframe = train_dataframe.iloc[test_indices]\n# Print the train and test dataframes\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\n# Get the number of rows in the dataframe\nn_rows = len(features_dataframe)\n# Calculate the number of rows to use for the train and test sets\ntrain_size = 0.8\ntrain_rows = int(n_rows * train_size)\ntest_rows = n_rows - train_rows\n# Split the dataframe into train and test sets\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=test_size, random_state=42)\n# Sort the train and test dataframes by date\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n# Print the train and test dataframes\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\ndef solve(features_dataframe):\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort([\"date\"])\n    test_dataframe = test_dataframe.sort([\"date\"])\n    # [Missing Code]\n    while test_dataframe.loc[0, \"date\"] <= train_dataframe.loc[-1, \"date\"]:\n        test_dataframe = test_dataframe.drop(test_dataframe.loc[0, \"date\"])\n        train_dataframe = train_dataframe.append(test_dataframe.loc[1:], ignore_index=True)\n    return train_dataframe, test_dataframe\ntrain_dataframe, test_dataframe = solve(features_dataframe)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndf = pd.DataFrame({\n    'Month': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],\n    'X1': [12, 10, 100, 55, 65, 60, 35, 25, 10, 15, 30, 40, 50],\n    'X2': [10, 15, 24, 32, 8, 6, 10, 23, 24, 56, 45, 10, 56],\n    'X3': [12, 90, 20, 40, 10, 15, 30, 40, 60, 42, 2, 4, 10]\n})\nscaler = MinMaxScaler()\ncols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].scaler.fit_transform(df[cols])\nprint(df)\n",
        "\ncols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].scaler.fit_transform(myData[cols])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nwords = load_data()\n# Initialize an empty list to store the feature names\nfeature_names = []\n# Loop through each word in the list\nfor word in words:\n    # Split the word into individual characters\n    char_list = list(word)\n    \n    # Remove any non-alphabetic characters from the list\n    char_list = [char.lower() for char in char_list if char.isalpha()]\n    \n    # Add the cleaned word to the feature names list\n    feature_names.append(''.join(char_list))\n# Convert the feature names list to a numpy array\nfeature_names = np.array(feature_names)\n# Fit the CountVectorizer to the feature names array\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform(feature_names)\n# Print the feature names\nprint(count.get_feature_names())\n",
        "\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names_out())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nGridSearch_fitted = load_data()\nassert type(GridSearch_fitted) == sklearn.model_selection._search.GridSearchCV\n# [Missing Code]\n# Get the results of GridSearchCV\nfull_results = GridSearch_fitted.cv_results_\n# [Missing Code]\n# Convert the results to a pandas dataframe\ndf = pd.DataFrame(full_results)\n# [Missing Code]\n# Print the dataframe\nprint(df)\n",
        "\n# Get the results of GridSearchCV\nresults = GridSearch_fitted.cv_results_\n",
        "\n# Save the model in the file named \"sklearn_model\"\nfitted_model.save(\"sklearn_model\")\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndf = load_data()\ntfidf = TfidfVectorizer()\n# Tokenize each description\ndf['description'] = df['description'].apply(lambda x: x.split())\n# Transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(df['description'])\n# Calculate cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = 1 - np.dot(tfidf_matrix, tfidf_matrix.T)\nprint(cosine_similarity_matrix)\n"
    ],
    "Pytorch": [
        "\nimport numpy as np\nimport pandas as pd\nimport torch\noptim = load_data()\noptim.param_groups[0].lr = 0.001\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\noptim = load_data()\noptim.param_groups[0].lr = 0.001\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\noptim = load_data()\noptim.param_groups[0].lr = 0.0005\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\noptim = load_data()\n# Set the learning rate of the optimizer\noptim.lr = 0.005\n# [Missing Code]\n# [Missing Code]\n",
        "\nembedded_input = torch.tensor(word2vec.wv.vectors)\n",
        "\n# Get the word vectors from the gensim model\nword_vectors = word2vec.wv\n# Convert the word vectors to PyTorch tensors\nembedded_input = torch.tensor(word_vectors.vectors).unsqueeze(0)\n",
        "\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n",
        "\nx = torch.rand(4,4)\npx = pd.DataFrame(x)\n# Convert the tensor values to numpy arrays\npx['0'] = px['0'].numpy()\npx['1'] = px['1'].numpy()\npx['2'] = px['2'].numpy()\npx['3'] = px['3'].numpy()\n",
        "\nx = torch.rand(6,6)\n",
        "\nC = B[:, A_log]\n",
        "\nC = B[:, A_logical]\n",
        "\nC = torch.LongTensor([[999, 777], [9999, 7777]])\nA_log = torch.ByteTensor([1, 1, 0])\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log]\n",
        "\nC = B[:, A_log]\n",
        "\nC = B[:, A_log]\n",
        "\nC = B[:, A_log]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nidx, B = load_data()\nC = torch.index_select(B, idx)\nprint(C)\n",
        "\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=np.object)\nx_tensor = torch.from_numpy(x_array)\n",
        "\nx_array = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n], dtype=object)\nx_tensor = torch.from_numpy(x_array)\n",
        "\nimport pandas as pd\nimport torch\nimport numpy as np\nx_array = load_data()\ndef Convert(a):\n    t = torch.from_numpy(a)\n    return t\nx_tensor = Convert(x_array)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\nmask = torch.tensor(np.zeros((len(lens), len(lens[0])), dtype=np.int64))\nfor i in range(len(lens)):\n    mask[i, :lens[i]] = torch.tensor(np.ones((lens[i],), dtype=np.int64))\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\nmask = torch.zeros(len(lens), len(lens[0]), dtype=torch.long)\nfor i in range(len(lens)):\n    mask[i, :lens[i]] = torch.ones(lens[i], dtype=torch.long)\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\nmask = torch.tensor(np.zeros((len(lens), len(lens[0])), dtype=np.int64))\nfor i in range(len(lens)):\n    mask[i, :lens[i]] = torch.tensor(np.ones((lens[i],), dtype=np.int64))\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\ndef get_mask(lens):\n    mask = torch.zeros(len(lens), len(lens[0]), dtype=torch.long)\n    for i in range(len(lens)):\n        for j in range(len(lens[0])):\n            if i < len(lens) - 1 and j < len(lens[0]) - 1:\n                mask[i, j] = 1\n    return mask\nmask = get_mask(lens)\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nTensor_2D = load_data()\nTensor_3D = torch.tensor(Tensor_2D).unsqueeze(0)\nprint(Tensor_3D)\n",
        "\n    diag_ele = np.diag(t)\n    ",
        "\n# Create a new tensor of shape (3, 11)\nab = torch.stack((a, b), dim=0)\n",
        "\na = torch.tensor(a)\nb = torch.tensor(b)\nab = torch.stack((a,b),0)\nprint(ab)\n",
        "\n# Create a new tensor of shape (3, 11)\nab = torch.stack((a, b), dim=0)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.rand((10, 1000, 96))\nlengths = torch.randint(1000, (10,))\na[ : , lengths : , : ]  = 0\nprint(a)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.rand((10, 1000, 96))\nlengths = torch.randint(1000, (10,))\na[:, lengths:, :] = 2333\nprint(a)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.rand((10, 1000, 23))\nlengths = torch.randint(1000, (10,))\na[:, :, :lengths] = 0\nprint(a)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.rand((10, 1000, 23))\nlengths = torch.randint(1000, (10,))\na[:, :, :lengths] = 2333\nprint(a)\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nnew_tensors = torch.stack(list)\n",
        "\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\nresult = torch.tensor(t[idx])\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\nresult = torch.tensor(t[idx])\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\nresult = torch.tensor(t[idx])\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nids, x = load_data()\nresult = x.gather(1,ids)\nprint(result)\n",
        "\nresult = x.gather(1,ids)\nresult = result.squeeze()\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nids, x = load_data()\nresult = np.argmax(x, axis=1)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\n# Get the highest probability for each input\nmax_probs = np.amax(softmax_output, axis=1)\n# Create a tensor indicating which class had the highest probability\ny = np.where(max_probs == np.max(max_probs), 2, np.where(max_probs == np.max(max_probs)-1, 0, 1))\nprint(y)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\n# Get the highest probability for each input\nmax_probs = np.amax(softmax_output, axis=1)\n# Create a tensor indicating which class had the highest probability\ny = np.where(max_probs == np.amax(max_probs), 0, 1)\nprint(y)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\n# Get the lowest probability for each input\nlowest_probabilities = np.argmin(softmax_output, axis=1)\n# Create a tensor indicating which class had the lowest probability\ny = np.zeros(softmax_output.shape[0], dtype=int)\ny[lowest_probabilities] = np.arange(1, 3)\nprint(y)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ndef solve(softmax_output):\n    y = torch.argmax(softmax_output, dim=1)\n    return y\ny = solve(softmax_output)\n",
        "\n# Get the lowest probability for each input\nlowest_probabilities = softmax_output.argmin(dim=1, keepdim=True)\n# Create a tensor indicating which class had the lowest probability\ny = lowest_probabilities.squeeze().detach().numpy()\n",
        "\n# One-hot encode the target labels\ntarget_onehot = torch.zeros_like(target)\ntarget_onehot[target == 0] = 1\ntarget_onehot[target == 1] = 1\ntarget_onehot[target == 2] = 1\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_not_equal = (A != B).sum()\n",
        "\nA, B = load_data()\ndef Count(A, B):\n    return np.sum(A == B)\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n",
        "\n# Get the last x elements of the two tensors\nlast_x = A[-x:]\nlast_x_b = B[-x:]\n# Check if they are equal\ncnt_equal = (last_x == last_x_b).sum()\n",
        "\n# Get the last x elements of the two tensors\nlast_x_elements_A = A[-x:]\nlast_x_elements_B = B[-x:]\n# Check if they are not equal\nnot_equal = np.any(last_x_elements_A != last_x_elements_B, axis=1)\n# Count the number of not equal elements\ncnt_not_equal = np.sum(not_equal)\n",
        "\na_split = torch.chunk(a, chunk_dim, dim=3)\n# [Missing Code]\n",
        "\na_split = torch.chunk(a, chunk_dim, dim=2)\n# [Missing Code]\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\n# Compute the minimum absolute values for each tensor\nmin_x = torch.min(torch.abs(x))\nmin_y = torch.min(torch.abs(y))\n# Compute the signs for each tensor\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\n# Compute the signed minimums for each tensor\nsigned_min_x = sign_x * min_x\nsigned_min_y = sign_y * min_y\n# Combine the signed minimums into a single tensor\nsigned_min = torch.stack([signed_min_x, signed_min_y])\n",
        "\n# Compute the maximum absolute values for each tensor\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\n# Compute the signs for each tensor\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\n# Compute the signed maximums for each tensor\nsigned_max_x = sign_x * max_x\nsigned_max_y = sign_y * max_y\n# Combine the signed maximums into a single tensor\nsigned_max = torch.stack([signed_max_x, signed_max_y])\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\n",
        "\nconfidence_score = torch.nn.functional.softmax(output, dim=1)[0, 0]\n",
        "\nresult = torch.cat([a, b], dim=1)\nresult[:, 1:] = result[:, 1:].mean(dim=1)\n",
        "\n",
        "\n# Create a new tensor with zeros\nnew = torch.zeros(1, 4, 2)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(4).reshape(1,2,2).float()\nnew = torch.tensor([[0., 0., 0.,0.]])\nresult = torch.stack([t, new], dim=0)\nprint(result)\n",
        "\n# Create a new tensor with the desired values\nnew = torch.tensor([[-1, -1, -1, -1], [-1, 1, 2, -1], [-1, 3, 4, -1], [-1, 5, 6, -1], [-1, 7, 8, -1], [-1, -1, -1, -1]])\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nhid_dim = 32\ndata = torch.randn(10, 2, 3, hid_dim)\ndata = data.view(10, 2 * 3, hid_dim)\nW = torch.randn(hid_dim)\nresult = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\n"
    ]
}